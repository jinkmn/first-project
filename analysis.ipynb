{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d741ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from function import *\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b4eb8",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9539d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2996 entries, 0 to 2995\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                                                  Non-Null Count  Dtype \n",
      "---  ------                                                                  --------------  ----- \n",
      " 0   申込区分                                                                    2996 non-null   object\n",
      " 1   申込区分（日本語）                                                               2996 non-null   object\n",
      " 2   ロール                                                                     2996 non-null   object\n",
      " 3   本日の総合的な満足度を５段階で教えてください。                                                 2996 non-null   int64 \n",
      " 4   本日の講義内容について５段階で教えてください。 \n",
      "学習量は適切だった                                      2996 non-null   int64 \n",
      " 5   本日の講義内容について５段階で教えてください。 \n",
      "講義内容が十分に理解できた                                  2996 non-null   int64 \n",
      " 6   本日の講義内容について５段階で教えてください。 \n",
      "運営側のアナウンスが適切だった                                2996 non-null   int64 \n",
      " 7   本日の講師の総合的な満足度を５段階で教えてください。                                              2996 non-null   int64 \n",
      " 8   本日の講師について５段階で教えてください。\n",
      "授業時間を効率的に使っていた                                    2996 non-null   int64 \n",
      " 9   本日の講師について５段階で教えてください。\n",
      "質問に丁寧に対応してくれた                                     2996 non-null   int64 \n",
      " 10  本日の講師について５段階で教えてください。\n",
      "話し方や声の大きさが適切だった                                   2996 non-null   int64 \n",
      " 11  ご自身について５段階で教えてください。\n",
      "事前に予習をした                                            2996 non-null   int64 \n",
      " 12  ご自身について５段階で教えてください。\n",
      "意欲をもって講義に臨んだ                                        2996 non-null   int64 \n",
      " 13  ご自身について５段階で教えてください。\n",
      "今回学んだことを学習や研究に生かせる                                  2996 non-null   int64 \n",
      " 14  親しいご友人にこの講義の受講をお薦めしますか？                                                 2996 non-null   int64 \n",
      " 15  【必須】本日の講義で学んだことを50文字以上で入力してください。                                        2996 non-null   object\n",
      " 16  （任意）本日の講義で特によかった部分について、具体的にお教えください。                                     875 non-null    object\n",
      " 17  （任意）分かりにくかった部分や改善点などがあれば、具体的にお教えください。                                   435 non-null    object\n",
      " 18  （任意）講師について、よかった点や不満があった点などについて、具体的にお教えください。                             371 non-null    object\n",
      " 19  （任意）今後開講してほしい講義・分野などがあればお書きください。                                        375 non-null    object\n",
      " 20  （任意）ご自由にご意見をお書きください。                                                    388 non-null    object\n",
      " 21  (任意) 今回がテスト運用となるチャットbotですが、今後改善をしていきたいと思います。気になる点、改善要望等あればぜひFBをお願いします。  227 non-null    object\n",
      "dtypes: int64(12), object(10)\n",
      "memory usage: 515.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"Day1_アンケート .xlsx\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32a5ea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>不明</td>\n",
       "      <td>student</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>並列で学習させる際の種類について</td>\n",
       "      <td>半導体の話がおもしろそうだと思えたこと</td>\n",
       "      <td>OmniCampusにZoomのリンクを載せる動線では駄目でしょうか？_毎回、動線が散漫とし...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>松尾研がどのような研究をされているのか知りたいです。</td>\n",
       "      <td>昨年資料にもありましたが、第１回目で各回の紹介は要らないように思います。</td>\n",
       "      <td>ぜひ英語表記を併記してほしいです！日本語表記になると訳文だとわかる日本語になってしまい、本当...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>不明</td>\n",
       "      <td>student</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>主に大規模言語モデル(LLM)の概要や講義の前半について学びました。LLMは単語列の生成確率...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>不明</td>\n",
       "      <td>student</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>大規模言語モデルの概要、今後学んでいくことの概要を認識できた。</td>\n",
       "      <td>今後の講義を外観できた。</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>初学者のため、理解は薄いと思いますが、今後の講義の全体感をつかめました。引き続き、よろしくお...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>学生</td>\n",
       "      <td>student</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>LLMの大まかな構造とこれからやっていくこと、LLM が世間ではどのように実装され実践的に活...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPONSOR</td>\n",
       "      <td>会員企業</td>\n",
       "      <td>student</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>・講座概要\\n・LLMの概要\\n　└次の文字列を確率的に計算しているモデルである\\n　└これ...</td>\n",
       "      <td>A先生の講義</td>\n",
       "      <td>各回の概要について。全体像を掴めたのは良かったが、1時間超かけてやるべきかは議論の余地がある...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>見逃しているだけだと思うが、講義内容は知識としての内容が多い印象なので、実際のアプリケーショ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1     2        3  4  5  6  7  8  9  10  ...  13  14  15  \\\n",
       "0      OTHER    不明  student  2  3  3  2  2  4   2  ...   3   1   5   \n",
       "1      OTHER    不明  student  2  2  2  2  1  1   3  ...   4   3   3   \n",
       "2      OTHER    不明  student  2  3  3  3  3  4   3  ...   4   2   5   \n",
       "3  EDUCATION    学生  student  2  4  4  2  2  3   4  ...   4   5   7   \n",
       "4    SPONSOR  会員企業  student  2  2  3  3  3  3   3  ...   2   2   2   \n",
       "\n",
       "                                                  16                   17  \\\n",
       "0                                   並列で学習させる際の種類について  半導体の話がおもしろそうだと思えたこと   \n",
       "1  主に大規模言語モデル(LLM)の概要や講義の前半について学びました。LLMは単語列の生成確率...                  NaN   \n",
       "2                    大規模言語モデルの概要、今後学んでいくことの概要を認識できた。         今後の講義を外観できた。   \n",
       "3  LLMの大まかな構造とこれからやっていくこと、LLM が世間ではどのように実装され実践的に活...                  NaN   \n",
       "4  ・講座概要\\n・LLMの概要\\n　└次の文字列を確率的に計算しているモデルである\\n　└これ...               A先生の講義   \n",
       "\n",
       "                                                  18   19  \\\n",
       "0  OmniCampusにZoomのリンクを載せる動線では駄目でしょうか？_毎回、動線が散漫とし...  NaN   \n",
       "1                                                NaN  NaN   \n",
       "2                                                NaN  NaN   \n",
       "3                                                NaN  NaN   \n",
       "4  各回の概要について。全体像を掴めたのは良かったが、1時間超かけてやるべきかは議論の余地がある...  NaN   \n",
       "\n",
       "                                                  20  \\\n",
       "0                         松尾研がどのような研究をされているのか知りたいです。   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  見逃しているだけだと思うが、講義内容は知識としての内容が多い印象なので、実際のアプリケーショ...   \n",
       "\n",
       "                                                  21  \\\n",
       "0               昨年資料にもありましたが、第１回目で各回の紹介は要らないように思います。   \n",
       "1                                                NaN   \n",
       "2  初学者のため、理解は薄いと思いますが、今後の講義の全体感をつかめました。引き続き、よろしくお...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                  22  \n",
       "0  ぜひ英語表記を併記してほしいです！日本語表記になると訳文だとわかる日本語になってしまい、本当...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_number = [str(i+1) for i in range (df.shape[1])]\n",
    "df.columns = column_number\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60e7776d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_columns = [\"17\", \"18\", \"19\", \"21\", \"22\"]\n",
    "comments_cell = df[comments_columns].values.flatten()\n",
    "comments = [v for v in comments_cell if pd.notnull(v)]\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c2753",
   "metadata": {},
   "source": [
    "# geminiのAPIキーを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40022ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07038cc0",
   "metadata": {},
   "source": [
    "### 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "660fc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 分析結果の例 ---\n",
      "{\n",
      "  \"classifications\": [\n",
      "    {\n",
      "      \"comment_index\": 0,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 1,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 3\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 2,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 3,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 5\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 4,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 3\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 5,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 6,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 1\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 7,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 8,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 5\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 9,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 4\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 10,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 11,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 12,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 13,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 14,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 15,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 16,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 2\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 17,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 18,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 5\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 19,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 20,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 5\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 21,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 3\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 22,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 23,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 4\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 24,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 25,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 26,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 27,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 28,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 29,\n",
      "      \"sentiment\": 2,\n",
      "      \"topic\": 2\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 30,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 1\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 31,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 32,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 33,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 34,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 3\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 35,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 36,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 37,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 3\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 38,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 5\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 39,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 40,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 41,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 42,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 43,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 44,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 45,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 46,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 47,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 48,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 49,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 50,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 51,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 52,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 53,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 54,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 55,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 56,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 57,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 4\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 58,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 4\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 59,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 60,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 61,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 62,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 63,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 64,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 65,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 66,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 67,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 68,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 69,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 70,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 71,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 72,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 2\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 73,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 74,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 75,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 76,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 77,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 78,\n",
      "      \"sentiment\": 1,\n",
      "      \"topic\": 1\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 79,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 80,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 81,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 82,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 83,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 84,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 85,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 86,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 87,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 88,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 89,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 90,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 91,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 92,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 93,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 94,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 95,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 96,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 97,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 98,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 99,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 100,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 101,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 102,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 103,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 104,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 105,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 106,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 107,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 108,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 109,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 110,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 111,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 112,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 113,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 114,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 115,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 116,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 117,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 118,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 119,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 120,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 121,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 122,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 123,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 124,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 125,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 126,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 127,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 128,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 129,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 130,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 131,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 132,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 133,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 134,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 135,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 136,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 137,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    },\n",
      "    {\n",
      "      \"comment_index\": 138,\n",
      "      \"sentiment\": 0,\n",
      "      \"topic\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"important_comments\": [\n",
      "    26,\n",
      "    27,\n",
      "    30,\n",
      "    58,\n",
      "    72,\n",
      "    78,\n",
      "    96,\n",
      "    109,\n",
      "    118,\n",
      "    128,\n",
      "    130\n",
      "  ],\n",
      "  \"dangerous_comments\": []\n",
      "}\n",
      "positive comments:['半導体の話がおもしろそうだと思えたこと', '今後の講義を外観できた。', '初学者のため、理解は薄いと思いますが、今後の講義の全体感をつかめました。引き続き、よろしくお願いします。', '単語列の生成確率のモデル化、これを基本としてモデルを育てているのがイメージしやすくて良かった。', 'どんな講義を受けていくのかについて理解ができた', 'コースの各回の概略を知ることができた。', 'NN やBERTあたりで止まっている私のAI知識をアップデートする深い内容だったと思います。LLMってAIの最前線なんですね。', '両方ともわかりやすくて、親しみやすいテイストでとっつきやすかったなと思います。', '導入のお話でわかりやすかった', 'llmを開発するための具体的な計算資源等への言及があったのが良かった。', '日本で開発している言語モデルがGPT-4などと比較してどの程度の規模なのかについてよく知らなかったので\\\\nイメージしやすいスライドがあったのがよかったです。', 'Bさんの講座。資料の流れが分かりやすく、講座の全体像が掴みやすかったです。', '来週以降への期待。', '前半と後半でスケーリング則に関して全く同じ資料を用いて同じ説明をしている点が気になりました。講座の回が異なるのであれば復習として同じ解説を行うことは良いと思いますが、同じ回の中では冗長に感じました。', '初回ゆえに丁寧にOverviewの時間をとっていただいた点は理解しておりますが、もう少しいきなり内容に入っても良いかとも思いました。', '丁寧な説明で置いていかれることなく受講することができました。', '初回というのも有り、全体として細部には踏み込みすぎず、理解し易い構成になっていたと感じます。', '社会人で、アーカイブにて学習をしています。スライドの作成に関しては、Googleスライドを活用し、それをそのままアップロードすることで、動画やアニメーションなどのコンテンツも含めて復習ができるので、来年以降の開催時に検討いただければと思います。', 'お二方とも、スピードや声のトーンが適切で、聞き取りやすかったです。', '私が直接利用をしたわけでは有りませんが、他の方の質問とその回答を見て感じたことを記載します。１：RAGが回答を引っ張りすぎて、質問の答えになっていないケースが有るかもしれない。ベクトルインデックスをQA形式（Questionにてクエリ）で保持するなどの対応が考えられますが、工数がかかるので必要に応じてご検討ください。２：回答内容が質問者のレベルに合っていない場合があるかもしれない。質問者の知識レベルを想定するCoTを入れるといった工夫があるかもしれません。以上、釈迦に説法であり、大変恐縮ながらもせっかくの機会なので提案をさせていただきます。', '各回の概要はもう少しコンパクトにしても良いと思いました。\\\\nLLMについて、それなりの前提知識があることが受講の条件になっているため、冗長に感じました。', '運営の事前の資料や説明が非常にわかりやすく、全く混乱なく当日を迎えられたことが素晴らしかった。講義の内容も、易しすぎず難しすぎず、初回としては良い導入だったと思う。アーカイブの公開も迅速で、受講時間の調整がしやすいので大変有難い。無償ということが信じられない。', '不満はなし。無償で講座を開いてもらい、ありがとうございます。', 'Slackのコミュニティの活発さに驚きました。参加させて頂きありがとうございます。', 'まだ使用していないので、使用してみます。', '全体像の解説がされていた点。', '全体像の解説においてマクロとミクロな情報が混在していた印象でした。今後受講するモチベーションになったが、現時点での説明としてはわかりにくい部分がありました。', '一般的な事前学習について、ドメインは限定でなくても既存のソースから学習している場合はすでに既存のバイアスが存在していることには注意が必要と思いました。', 'このチャットbotはGENIACで作成したものですか？', 'LLMの現状の概観を掴めて良かった。', 'みなさん声が聞きやすかったです。', '「日本のLLMを取り巻く環境」で日本の開発状況を知れたことがよかった。', 'モデルをスケールして学習させる場合、データ並列以外にも、モデル並列と呼ばれる手法があることが分かり、複数のGPUがあるならば、うまく活用することで効率的に学習させることができるということを知ることができた', 'LLMの基盤モデルと言う考え方がなぜパラダイムシフトになっているのかといったことがわかってよかった。(LLM以降に機械学習を触った人間なので当たり前だと思っていたが、歴史を知ることでなぜこれが次のブームになっているのかをしれた)', '体系的にLLMの動向が分かって良かったです。', '内容がかなりあったので、ちょっと復習したい感じです。', '皆さんとても分かりやすく工夫して話されており良かったです。', 'ありがとうございました。GENIACの時の不完全燃焼だった部分を、こちらで基礎を抑えながら回収していきたいです。', '分かりました。', '言語モデルの定義から入ったことで、概論的な技術感や戦略、展望に着いていくことができた。', 'コンテキスト長の問題の解決について、単語埋め込みなどに触れても良かったのではないか。', '他人のやり取りの中でいくつか誤答や混乱を招きそうな解答が見受けられたので、フォローが必要かもしれません。（例えばQ97、98）', 'あまり予習をしていなかったため、概要を聞けてよかったです。', '2時間休みなし、は少し疲れました。前後半の間に5分ほどの休憩を挟んでもよかったかもしれません。', '内容が入門的だった為、概論と各項目の関係を理解しやすかった', '実際にどう言ったサービスに応用されているか？と言う点で実際に稼働しているサービスの例など交えて説明頂けると良かったかなと思う', 'LLMの精度を求める指標の一つとして、事前学習中の Loss があるという話があったが、昔チャットボット用のモデルを構築していた時は\\u3000Perplexity（次の単語の選択肢の逆数）があった。今はその指標は使わないのか知りたい。', '講義全体を概観するには，各々の内容が長すぎる，一方で結局1-2ページ分しか触れられてないのでトピックの本質はわからないモヤモヤが残りました．\\\\nしたがって，概要説明はもっと簡易で，具体的な内容に時間を使ったほうが良いかと個人的に感じました．', '資料を見ておいて下さいと言われた部分が資料としてアップされていない点．具体的には，計算量の簡易的なモデルの議論についてのスライドです．', '分かりにくいというか、第2回でMeta社のLlama3を使っての演習があるそうですが、どの程度演習をやるか、そのレベルはどのくらいかを知りたかったです。\\\\nまた、コンペについて、今年何をやるかはまだ決まってないかと思いますが、前回はどのような内容でどのような作品が提出されたかは聞きたかったです。コンペは提出が必須なのかといったことも。', '延長なく終わったのがまずよかったです。\\\\n声とかも聞き取りやすかったです。', '運営は大変かと思いますが、こちらとしては感謝しかありません、ありがとうございます。', '回答もそうなんですが質問もレベルが高いのから低いのまであって、ちょっと見づらいかもしれないです。もう少し厳選するような仕組みがほしい。', 'LLMの活用法（PromptingとRAG）', '今回分の話は知っていたので、Day0にしても良いと思いました。', 'どんな難しい、専門的な話が展開されるか心配していましたが、初歩的な説明や配慮が感じられ、嬉しかった。もちろん、これからハイレベルな講義になっていくと思うので、油断はできないのですが。', '話すスピードや滑舌、「慣れていらっしゃる」んだなと関心しました。', '前年からLLMを取り巻く環境も変わり、それを踏まえて講座を前年からアップデートしてもらえていることにこれからの期待感が高まりました。', 'アンケートについて1~5でどちらが「Yes」なのかが明確ではないと思います', '鼻をすするようなノイズが終始聞こえたのが気になりました。', '今の開発の現状が知れたこと。', '具体例などがあるとよりわかりやすいです。', '聞きやすかったです。ありがとうございます。', '今回は使用しませんでした。今後、使用しここに記したいと思います。', 'テストロスってなに？', '大規模モデルの具体的なパラメータ数や日本のGPUリソースの整備状況などについても解説があり，世界と日本との具体的な差分がよくわかった．', '所々で講義内容を整理する時間があったので、助かりました。', 'これからの授業で話す内容の言葉の意味や概要を軽く知ることができて、興味がさらに湧きました。', '予習内容を軽く説明して欲しかった', '前回のLLMの講座は受講していませんが、今回は日程も長く内容も盛り沢山ということで、多くのことが学べる予感がしています。全体概要の説明でその点を知ることができたのは今後の参考になりました。', '特にありません。', '説明が聞き取りやすく、わかりやすくてよかったと思います。', 'Overviewで講座の全体像を知る感じですと若干難しそうな印象を受けますが、なんとかついていけるように努力したいと考えています。', '利用していないのでなんとも言えないですが、今後活用していきたいと思います。', 'Tanukiの紹介が結構なされていたところ。\\\\n初学者に向けてうまく今後の授業日程と絡めて、全体像を説明されていたところ。', '例で紹介されていた行動系列の生成についてはロボットのできることが増えていることと、純粋に面白いなと思いました。', 'LLMが今注目されている理由やこれから学ぶことの全体感を理解することができました。基礎からその応用を学ぶ絶好のタイミングだと思いこれから頑張って勉強しようと思います。', '各回の解説だけでも、一部知らなかったような話がいくつもあり、今後の講義が楽しみになりました。', '図が豊富でこれまで学んだことも復習ができました。', '回答が一般的な回答と本講義についての回答が混ざっているように感じるので改善してほしい。', '事前に資料が共有されてよかったです。', '分かりにくいわけではないですが、一部情報が古いように見受けられました。', '丁寧にお話しいただきました。', 'RLHF & Alignmentの中で、最近、見る機会が増えたDPOも触れ頂きましたが、座学だけでなく演習をご用意いただけるとありがたいです。', '全体像を説明されたので、理解の助けになりそうと感じた。', '講義時間が長めに感じた', '丁寧に説明されていたと思います', '面白い試みだと思いました。ハルシネーションをどれだけ防げるのかが気になります。', '第一回目とということで各回の概要について簡単に触れていただいた点です。これにより、全体の概要の理解と今後の授業のモチベーション向上に繋がりました。', '数学的な側面についての説明が少し速すぎると感じました。もう少しゆっくりと詳細に説明して欲しかったです。また、用語に対する定義や前提知識が不足している部分があり、事前に用語の説明や関連する基礎知識に触れる時間があれば、より理解が深まると感じました。', '講師のB先生は、豊富な知識と実例を交えた説明で、講義を非常に引き込まれるものにしていました。質問に対しても丁寧に答え、受講生の理解を確実に深めようとする姿勢が見られました。ただし、いくつかの専門用語については、より詳しい背景説明があれば理解しやすかったかもしれません。', '全講義の概要を説明してくれたので、見通しが良いと思った。', '最終課題のコンペについて知れたこと、講義全体の構成を知れたことは良かったです。', '内容的には既知のものが多かったので、特にございません。', '声が聞き取りやすく、解説も理解しやすかいと感じました。', 'GCI2023 Summer修了後、再び、松尾研で学ぶ貴重な機会を与えていただき大変感謝しております。', '何件かQ＆Aを覗いてみましたが、引用論文が示されているのは良いと思いました。特に問題点はなさそうです。', '・スケール則自体やその経済的意味について初めて知れたので、良かったです。', 'Aさんの声がごもごもしていて聞き取りにくかったです。Aさんに限らず今後の講師の皆様にもお願いですが、Zoomだとどうしても音質が落ちるので、普段よりも意図的にハキハキ喋っていただけると大変助かります！', 'ちゃんと回答がされていたのですごいと思いました！１つ気になったのは、他の方の質問に対しても、GoodボタンやBadボタンを押せそうなUIに見えるのですが、押してもいいのでしょうか？', '東大メタバース工学部の講義全般について感じていることですが、話のフックが弱い印象があり（平坦？）、初学者の私にとって難しく感じます。講義内容をアップデートされているとお話されていましたので、今後を楽しみにしています。', 'Q＆Aが非常にわかりにくいです。運営確認中という質問もあり今年度はスキップされることなく全てに対応されている点は素晴らしいとは思います。QA自体も補足的学習になりますのでエクセルで全体を見られるようにしていただけると大変ありがたいです。', '昨年の講座との違いをわかりやすく説明いただけました', '昨年の論文のボットは大変ありがたかったです。今回は、ぜひ、問いと生成結果をEXCELに転記する仕様にしてください。', 'LLMに至るまでの歴史的な変遷は知らなかったのでためになった。', '半導体について学べる機会があるのはとてもうれしく楽しみです。', '音声に聞き取りにくさがあったので、良質なマイクをお使いいただけると大変助かります。', 'LLMをなぜやる必要があるのかを冒頭に説明してもらえたことがよかった。今後のモチベーションに繋がった。', 'とても聞き取りやすい速度、トーンで分かりやすかったです。', 'Transformer登場までの概要を知れたのがよかった。\\\\nまた、各回の概要についても事前に触れておくことができたため、受講時の理解度が深まりそう。', '今後の学習ロードマップをとおして自分が予習が必要な箇所を認識できた点が非常によかったです。\\\\n', '今回の講義においては特にありませんでした。\\\\n\\\\nもし今回の内容が昨年資料の内容であった場合、理解できない箇所が多かったと思います。\\\\n入口としては今年の資料の内容でよかったと感じています。', '全般的に理解しやすく、また今後準備が必要となる部分がわかり、（今後の）講義を受けるうえで非常に役に立ちました。', 'これからよろしくお願いします', 'LLMの基本的な内容から説明くださり、関連知識がおさらいできて良かった。', '汎用LLMの事前学習に計算機資源がどの程度必要なのか知ることができて大変興味深かったです。', 'scaling lawのあたりで、量を増やせば一気に質が変わるという旨の話がありました。画像診断AIでは内挿という限界があったと記憶していますが、LLMでは量を増やすことで内挿の外も対応可能になったということなのでしょうか？ ', 'よく出てくるが理解していなかったグラフや用語を理解できた。\\u3000日本と米国の進捗状態、勝つための方策＝”新しいものは性能が伸びる、消費電力が下がる、なので今ここで力を入れれば巻き返せる。”\\u3000ピンと来たのですが、他国も圧倒的な量のGPUを使って研究開発をするので他国にもできるかなと思うところがあります。まず、遅れないこと、国策としての日本語LLMを強くできればと思いいます。またフォロワーに徹して、ファインチューニング部分でいろんなドメインで勝つことが大事で、企業ユースなど付加価値は、ファインチューニング部分のほうが応用上は70％～80％になるような世界になれば、ファンダメンタルモデルをつく津琴乃後れを取り返せる。そのような戦略とビジョンで国全体で動くことがより現実的と思われます。\\\\n', '特によかったことに意見を書きました。', '限られた時間で私のような全くの素人に対しても、前述の原理や使用されている技術、公開されているLLMの現状と今後新たなLLMを開発する上での問題点等を俯瞰できるようによく練られた構成だと感じました。', 'わからないことだらけですが、個々の事例はこれから詳説いただけると思いますので、特にありません。\\\\n強いてあげるなら、スライドを説明される際のポインタがもう少しはっきりと見えると説明と連動してよりわかりやすくなると感じました。', '喋り方、スピード、内容に関して不満はありません。', '先に各講義の概要が聞けたので、本応用講座の内容についてイメージつきやすくなった。', '特になし', '全体像を把握できました。', '特にありませんでした。', 'お忙しい中時間を割いていただきありがとうございます。これからよろしくお願いします。', 'stanfordのCS25の日本版のような印象を受けました。これから受講するのが楽しみです。\\\\nhttps://www.youtube.com/watch?v=fKMB5UlVY1E&t=1028s', 'NN、FineTuning、Promptingの比較', '質問が内容ごとにタグ付けされて分類される仕組みになればいいなと思った。', '学習データ、モデル、コンピュータリソースのスケールさせることがLLMの性能向上には重要という内容が印象的でした。', 'スライドで英語で書いてある部分があったので、できれば日本語でお願いしたいです。', '難しい計算式がありつつも、ある程度抽象化した表現で、LLMがどういう仕組み、原理になっているのか説明が非常に分かりやすかった', '後半パートで、最初の方の音声が少し滑らかでなかったこと以外は特にありません', 'オンラインイベントとかで、交流できる機会が今後あると、いい意味でLLMの領域におけるネットワークが広がると思いました', 'モデルの学習に必要な計算量が概算できるというを知れたのはよかった。\\\\n', 'モデルの学習に必要な計算量が概算について、講義でちゃんと説明して欲しかった。', 'これからの講義期待しています。', 'LLM の概要については、これまでにも耳にする機会があったが、具体的なグラフや数値でデータを示してくれた点がよかった。', '起源の部分に関しての学習ができてよかったです。', 'アジェンダの説明については軽く流していただくだけの方がスムーズだったかと思いました', 'とてもご丁寧にわかりやすかったです', 'ありがとうございました\\\\n次回もよろしくお願いします', '講義を通じて、LLMの構造について具体的なイメージができたことで、これがどのように世界で必要とされ続ける技術であるかをより理解できました。特に、LLMが単なる言語処理だけでなく、様々な分野で応用可能な汎用性を持つことや、スケーラビリティによって解決できるタスクの幅が広がっていることが印象的でした。\\\\n\\\\nまた、TransformerのSelf-Attention機構が、言語の文脈を深く理解し、より正確な出力を可能にしている点も、LLMが他の技術とは一線を画す理由として納得できました。これらの理解を通じて、LLMがただの技術に留まらず、将来にわたって世界中で様々な問題解決に寄与し続ける存在であることを強く実感しました。', '専門的な言葉がたくさん出てきて、意味がわからないところがありました。もっと簡単な言葉で日本語で説明してくれると助かります。', '特にないです。', '今回の講義を準備してくださった先生と講義者の皆さん、本当にありがとうございました。講義のおかげで、新しいことをたくさん学べてとても楽しかったです。丁寧に教えていただき、わかりやすく説明してくださったおかげで、難しい内容も少しずつ理解することができました。\\\\nこれからも学びを続けていきたいと思いますので、また次の事前準備し、次回講義も楽しみにしています。改めて感謝の気持ちを伝えたいと思います。\\\\nありがとうございます！', '次回からの講義も楽しみにしています！', 'ただの質問ですが、データを集める際に「データの質」という言葉があったと思います。このようなデータの質について定量的に評価するような研究等はあるのでしょうか。\\\\nデータの質を上げるような操作があればデータ数を増やさなくても学習精度が上げられるかもしれない等。', '講義のプレビューといった内容で、次回以降が楽しみになりました。', '2時間は集中力が持ちませんでした。途中で休憩が欲しいです。', '初心者でも体系的に学べそうだと言う点がよかったです。', '仕方のない部分でもあり、もっと勉強をしておけよ。という観点もありますが、専門的な用語が比較的よく出てくるので、とっつきにくさはありました。', 'とっつきにくいイメージがあったのですが、この内容を日本語で気難しくなく伝えていただけたことが、これからの活躍するであろう若者の門徒をひらくのではないかと思いました。', '貴重な機会をいただきありがとうございます。\\\\n\\\\nこれまで運用されてきたslackのチャンネルやコミュニティーを活かしていこうという狙いは十分に理解できたのですが、当選の通知から初回のの講義までに準備しなければいけないことの情報量が多すぎて、何度も離脱しそうになってしまいました。今後会を重ねるごとに参加率（というのがいいのでしょうか）はどうしても悪くなってしまうと想像しますが、当選から初回講義までを簡単に（いまも簡単になっていますが初見では情報量が多い）できると当選からの初回参加率は改善できるように思いました。\\\\n\\\\nただでさえ濃い内容を無料で開放していただいているのにスイマセン。こういった学びの機会が等しく提供されることに凄まじい可能性を感じますし、これから教育のあり方も変化していくと思います。ひとつの意見としてあげさせていただきました。', 'この講義のなのでAIを活用しながらチャットボットを運用されているんだろうな。と想像できたため特に気になりませんでしたが、実際にいまの状態で実運用には耐えられないだろうなと感じました。人はチャットボットに回答されたいわけではないので、チャットボットのような印象がtanuki-8Bらしい人肌温まるような回答を期待しています！', 'LLMに関して日本の取り組みが分かりやすかったでした。', '今後のカリキュラムで学ぶことが事前に知れたこと', 'LLMと他のモデルとの違いが「モデルを固定できること（Pre-train,Prompt,Predict）」にあり、それはパラダイムシフトといえることが、今後の学習意欲を高めるうえでとても印象に残りました。', '講義の構成として、これからの学習に向けて意欲を掻き立てられる良い時間でした。これからも宜しくお願いします。', '各回の概要を説明いただいたことで、予習をする際の方向性の参考になった。', '第一回ということで今後学んでいく内容の概要がしれたのは非常に良かった。全体を俯瞰することによって今どの部分を学んでいるか見失わなくなるので。', '今後の講義のイメージを掴めたこと\\\\nGPUの価格や計算スケールなど、知らない情報を教えてもらえたこと', '半導体の話まで聞けるとのことで、楽しみにしています。', '基礎部分も変化に合わせてアップデートされるとのこと、大変ありがたく思います。教材作成お疲れ様です。', 'LLMを学ぶ意義や注目されるようになった要因について把握できるよう講義構成が考えられていたのが良かったです。', 'なぜLLMが今注目されているのかをScaling Lawによる正確性の向上などを例に説明して頂き、何故多くの会社がLLMやAI半導体に対して思い切った投資ができるかが腑に落ちました。\\\\n', 'とても丁寧で分かりやす説明で、初学者の自分にとって大変助かりました。', 'スムーズな説明で理解がとてもしやすかったです', '重なりますが、スムーズな説明で理解がとてもしやすかったです', '全体感を一気に学べて良かったです！', 'モデルの学習に使われた GPU の台数や費用など、具体的な数値を交えた説明は特にイメージが湧きやすくて興味深かった。', 'LLMの全体像をわかりやすく俯瞰できたのがよかったと思います。', 'ございません。（わかりやすかったです）', '難しい内容も多かったと思いますが、理解しやすいように簡潔にまとめていただいていたのがわかりやすくて非常によかったと思います。', 'このような学びの機会をいただき、本当にありがたいと思っています。講師のみなさまにはご負担も大きいとは思いますが、ぜひこのような活動を続けて、広げていただきますと、日本全体の発展にもつながると期待しております。', 'まだ十分に利用できておらず、何か気づきがありましたらお伝えしたいと思います。', 'GPT3, GPT4のパラメータ数、学習データ量についての説明が', 'コマンドラインがなぜ出力に影響を与えてなぜ言葉を選ぶと出力が良くなるのか？例を与えると良い出力が得られることとの類推的な理論を教えて欲しかった。計算してコマンドラインの言葉を何が最も良いのか変分法にて求められるのではないのでしょうか', 'どなたの講師様がどなたかお名前を忘れてしまったです\\\\n高地の電波の届かない場所から急いで下界に降りてきたので創作自動車の中で講義を聞いていました\\\\niPhoneの画面が小さ過ぎた！', '格安simですのでZOOMの画面が途切れてカクガクして話が途切れ途切れになった\\\\n\\\\n専用通信端末が欲しくなった', '初回で全体像が見えたことは良かったです。また、講義資料に論文タイトルを載せていただいてる点も良いなと感じました。', 'ハードGPUの資源の観点からLLMの学習について説明されていたのが良かったです。', '今後の講義で扱うトピックの概要が把握でき助かりました。', 'これからの学習内容を細かく説明してもらえたことで、何のための学習なのかを考えながら受講できると思えた', '今日配布されたPDF資料と講義で使われている資料とで差異がありました\\\\n当たらに配布されるのでしょうか？', 'モチベーション高く頑張ろうと思いました！課題の内容がどういったものになるのか早く知りたいです。引き続きよろしくお願いします。', '今期受けることのできる講義の概要が期待の持てる内容で、来週からの講義が楽しみになった。', '今わからなくていい部分は分からなくていい、と言っていただけたので理解しやすかった。', '社会人として参加する時間の確保が大変だったですが、なんとか後でのアーカイブで学べました。素敵なシステム＆運営に感謝しております。', '講義各回で学べるであろう内容がわかって、学習計画全体像が把握できた', '私は自己回帰モデルに興味があるので，もっと詳しく説明してほしかった．なぜ，1トークンずつ出力するのが良いのか？一気に全てのトークンを生成する方が効率が良いと思うので説明してほしかった．でも今回は概要なので内容は適切だったと思います．', '講師の方の話し方が聞き取りやすくてとてもよかったです！', 'これから詳しい話などを聞けるかと思うので特にないです。', '特にないです。', '授業がぴったりに終わったのが、素晴らしいと思います。詳細を学ぶ前に今日のような説明があると、予習がしやすくて良いと思った。', 'ITエンジニアから研究者になった方の話を聞いてみたいと思った。', '講義資料含めてとてもよく準備させていると思いました。お時間を割いていただいたかと思います。素晴らしい学ぶ機会を与えてくださりありがとうございます。', '理論の説明だけなく、実際の活用例と結びつけながら説明していただけたので理解しやすく、興味を持って講義を聞けました。', 'Transformerの仕組みについては理解しきれなかった。', '特になし。', '引き続きよろしくお願いいたします。', '毎回のコースについて事前に把握することができた。', 'RLHFとDPOの違いについての説明が不十分でよくわからなかった。', 'よかった点は、あらゆることにたいして丁寧に説明して貰えたこと。\\\\n不満があった点は、その裏返しで時間がとても長くなったこと。', '事前に講義の時間を教えてほしい。', '事前に講義の時間を教えてほしい。', '大学の講義なので技術的な背景の解説が手厚く、LLMの原理から学べた\\\\n今後の講義への期待感が高まった', '手を動かすパートがなかったので若干メリハリにかけてしまい、集中力が途切れることがあった', 'わかりやすかったです', 'LLMの本質的な話が良かった。GPTなどは質問への答え方や振る舞いがとても良くできているので、文章の予測でここまでのことができることに特に驚いた。人の脳だとどうだろう、とも考えてみた。人の場合は、文の「続きを予測」をすることと、与えられた質問に「答える」ことは全く別のことをしているはずだと直感的には思っていたのだが、改めて考えてみると、我々も全体的なコンテクストから答えを出しているわけで「つぎに出すべきものを予測している」というフレームで捉えることもできるなと思った。', 'これからの大きな学習の流れを理解することができた。', 'パラメータと処理能力の関係。なぜ特異点がそこで起こるのか。何が起こっているのか。', '聞き取りやすく、わかりやすい説明でした。ありがとうございます。', 'とてもいい感じで自然でした。', '世界からみると日本は周回遅れだが、後発であるがゆえに、半導体性能アップが活かせたことができそう。', 'ハルシネーションは、減らすことできても、なくすことはできるのだろうか。ハルシネーションの活かし方。', '丁寧に説明いただき、良かったです。', 'とても楽しみになりました。よろしくお願いいたします。', '現時点では、要望はありません。ありましたら随時FBさせていただきます。', '講義の全体感と、\\\\n抽象度の高いレイヤーでLLMに関するトピックを理解できたのが良かった', '今回に関しては特にない', '今回に関しては特にない', 'Pre-training、RAG、事後学習の仕組みがあることを知れて良かった。事前学習である程度データ量を増やすことができればそれを土台としてRAG、事後学習を組み合わせて必要なOutputsを出せるのかなと思った（認識誤っていたらすみません）。そしたらいま自分がやりたいことができると思った。', '講義を受ける前にある程度自分で勉強していたものの、知らない単語や論文ベースの話になるとわからなかったので勉強しないといけないと思いました。', '話がわかりやすくて、ゆっくりで、例え話も用いてくださり良かったです。不満はとくにありません。', 'チャットボットがどれを指しているのかわかりません。すみません。', '講座全体の概要を知ることが出来て、受講のイメージが明確になりました。', '資料のフォントが統一されてると読みやすいように思いました。', '熱心に説明いただき、たくさん伝えたいことがあるという熱意が伝わって来て非常に良かったです。', 'GCI 2024 Winterの受講対象者に教職員（大学）拡大されれば、そちらも受講したいと思っております。', '講座の中で活用して参りますので、気づいたことはFBいたします。', '日本のLLM開発の状況について豊富な具体例をともなって説明いただいたため、\\\\nかなり理解することができて、とてもよかったです。', '全体的に興味が湧く内容だったかと思います。', 'zoom live的な講義だと、つまずいたらアーカイブが上がるまで確認できず、\\\\nyoutubeなどの動画講義に慣れているため、少し理解に不安があります。\\\\nその場で質問回答しないのであれば動画配信でいいのでは？と思ってしまいました。', '前半の基礎的な概念の再学習。言語モデルの概念など。', '今回は概論と割り切っています。特になし。', '特になし。ですが、質問への回答が今後どのように見れるのかは、またアナウンスなどがあるとありがたいです。', '講義全体と各回の概要をあらかじめ簡略にまとめておいてもらえたところが大変助かりました。', '今後ともよろしくお願いいたします。', 'LLMの成り立ちと構造の説明がとても参考になりました。', '・質問やディスカッションタイムがなかったのは残念でした。\\\\n・初回の授業の手引きがメールで送られていることが分かりにくかったです。Slackの項目を探したり、omnicampを探したりしてしまいました。掲出箇所の明記をしていただくとありがたいかなと思いました。\\\\n・個人の勉強利用の範囲での録画はOKにしてほしい。終わった直後に復習をしたいので動画が上がるまでのラグが長い。', 'よろしくお願いします。', '詳しい講義に入る前に、全体像を把握できたのが良かったです。森を見てから個々の木々を見ていくことが出来そうです。', '今日の講義内容では、特に分かりにくい点はなかったと思いますが、第8回の「Advanced Pre-training」のデータ並列やモデル並列の内容が難しいと思いました。', '音声は聞き取りやすくて良かったと思います。', '日本でトップを走っている研究室のAIに関する講義に参加することが出来て大変うれしく思います。今後ともよろしくお願いいたします。', 'まだ使用していませんが、今後使用していく中で何か気づく点があれば要望を出していきたいと思います。', 'ざっくりで良いのでタイムスケジュールが見えるとよかったです。', '充実した講義を受講することができるので、毎週水曜日が楽しみです。', 'リンクボタンがわかりづらいので、①配置場所を「大規模言語モデル 2024」の下に配置したり、②ボタンサイズを「マイページ」より大きくして、目立つようにしたらよいと思いました。', 'LLMの学習からチューニングまで全体像がある程度理解できた。', '次回以降細かく学ぶので、初回は理解しにくい部分はあるが仕方ないと思う。', '参考資料として挙げられているのが英語論文ばかりですが、専門外で読む時間がないので、日本語のテキストや資料もなるべく紹介してほしいです。', '情報が詰め込まれ過ぎてるような気がした。何ヶ所か早口になって、内容がちょっと追いつけない所があった。', 'ZoomアプリとWebブラウザの切替ながらの入力となるのがちょっと煩わしいです。', 'LLMの概要を理解できたこと。', '特に後半のB先生の講義は、全体的にわかりやすく、簡潔な言葉で説明していただけたので、非常にスムーズに理解できました。RAG、Transformerモデル、RLHFに関しては、予習した段階では難しく感じていましたが、今回の講義を通じて概略をつかむことができました。特に、図における矢印の流れやプロセスの解説がとてもわかりやすく、大変助かりました。', '改善点を1つ挙げるとすれば、早口で聞き取りにくい部分があったため、再生速度を1未満に設定できるオプションがあると助かります。また、可能であればもう少しゆっくり話していただけるとありがたいです。聞き取れない箇所があると、内容を理解しきれなかったり、メモを取り損ねてしまうことがあったためです。', 'もう少しゆっくりお話しいただけると、さらに理解が深まると思います。お二方の先生の説明は非常にわかりやすく、自分が理解できていなかった箇所や知識の不足に気づかせていただきました。その結果、自覚のなかった不足部分をしっかり学ぶことができ、大変有意義でした。ありがとうございました。', 'ありがとうございました。', 'チャットボットそのものではなく、表示に関する要望です。他の受講生のQ&Aが学習に非常に役立つため、Q&Aの並び順を時系列で昇順・降順に選べる機能を追加していただけると助かります。また、Q&Aを選択して閲覧した後に「戻る」を押すと最初のページに戻ってしまう仕様も、元のページに戻れるよう改善していただけるとありがたいです。', 'Aさん、Bさん 共に、リアルで最前線の研究者が注目している論点が本人の口から伝えられるところから、LLM全体像の中の何が幹となるのか？（または、幹と考えられているのか？）というストーリーが見えるところが良かった。', '図表が多いことが特によかった部分だと思います。わかりやすいからです。', '技術変化の流れについても教えていただければと思います。今使われている技術は無から生まれたのではなく、過去の現に淘汰された技術から生まれたと思います。流れという背景を知れば、今の技術の良さと悪さを更に理解しやすいではないかと思います。', '当日の使用された資料の一部が配布した資料に含まれていなかったため、更新をお願いいたします。', '講義の概要で体系的にLLMの学ぶべきポイント一覧を理解しました。', '特にLLM開発時の学習データの収集における注意事項や評価タスクについてまとまっておりためになりました', '特にありません', '特にありません', '特にありません', 'まずが利用して回答いたします(ほかの方のQAが見えるのは良いと思います)', '基礎的な部分でも説明を入れてくれることで、理解できないということが少なくなり、離脱をせずに通して受講できた。\\\\n全講義の簡単な説明とはいえ、かなりボリュームのある内容だったかと思いますが、興味を引く説明をしてくださっていた。', '声が小さく聞き取りづらかった', '「Say-Can-PaLM」「RT−1」の取り組みについては存じ上げず、実世界応用ができる世界観が広がる機会となりました。\\\\n「日本のLLMを取り巻く環境」については、定点観測できたことで、発展目覚ましい日本の状況を驚きをもって感じることができたと同時に課題感も感じた。', '全体的に触れる内容だったので、大きな不満や改善点はないと思います。\\\\nありがとうございました。', 'この 1 回で全体の講義の流れを掴むことができたためよかったです。今後の予習を行うときにも役に立ちそうです。', '特にないです。', 'とても理解しやすく良かったです。', '各パートについて、端的にまとめられていて、飽きが来なくよかった。', 'ちらっと見たたけですが、Tanuki-8X8Bの応答が既存のLLMモデルと比べ、とてもこなれているところに感心しました。', '今後の各講義の中身の概要よりも、その前段の自己回帰言語モデルからTransformerの流れ、昨今のLLMの状況の流れが理解しやすくてよかったです。あと、Pre-trainingについてあまり調べたことがなかったので、Lossをどう捉えているのかがわかりました。', '特にありませんでした。', '本日は3部構成だったと思いますが、3部目で今後の講義概要＋少し触りの紹介があったのがイメージが湧いてとても良かったです。', '皆さんとても良かったです。', '(1) UX的にchatとしてやり取りできるのではなく一問一答であり、そしてアーカイブされていくようですので、もっと長めに出力されてもいいかもなと思いました。(2)Perplexityのように、関連する質問をリコメンドしてくれたら、その場でそれをたどっていくことで、その瞬間の学びの量が増やせそうです。', 'これからどのように講義が行われていくかの全体像が理解できたこと。', '途中でトイレ休憩がほしいです。', 'とくになし', '概要を幅広くいい意味で薄く広くご紹介いただいた点', 'EmergentAbility ：一定の学習を超えると問題解決能力が閾値を超えるというLLMの振る舞いにおいて我々人間でも同じようなことが起きてるのか？と興味をそそられたため。\\\\n事前学習：必要な計算リソース、データセットとモデル開発、手順について。\\\\nFineTuningとRLHF：どのようなフィードバックやプロセス、評価ポイントを用いているのか。有用、誠実、無害かという３つの観点。\\\\n半導体や計算資源：世界と日本の差やLLMを開発運用するための必要なインフラの規模について', '今日は概要の理解なので具体に深ぼれなかったということ以外に特にありません。', '夜は集中できないので、次回からアーカイブで見ます', 'LLMの基本的な事について分かりやすく解説頂きました。', '各回の概要については、その回で深く説明した頂けるのであれば、もう少し概要だけの説明に留めていただければ時間が節約できたと思います。', '丁寧にかみ砕いて説明頂いたので分かりやすかったです。', '大変勉強になりました。', '今の所、気が付いて点はないです。何かあればご連絡します。', '次回以降の講義で話される内容についてのの概要を知ることができたので、\\\\n次回以降の予習がやりやすくなりました。', 'これから授業でやることに触れていただいたことです。\\\\n何につながっていくのかが分かりながら授業が受けれそうで楽しみになりました。', 'LLM の概要を理解することができました。しかも一般レベルよりやや深い内容だったので良かったです。', '確率分布関数、連鎖律が何かはよくわかりませんでした（分からなくても講義は理解できていますが）。', '学会発表ほど深すぎず、一般向け講演ほど浅すぎず、ちょうどよいレベル感でした。話も聞きやすく、簡潔でよかったです。', '講義全体の見通しが立ったのがありがたいです', '特にありません', '特にありません', '4,000人を超える受講者のアンケートをどのように整理しているのか気になります。\\\\n特にこの欄などの自由記述について、整理するに当たって生成AIを活用されたりしているのか知りたいところです。', '数値や学術論文からのデータをエビデンスとして提示いただけたこと', '人数が多いため、つぶやき\\u3000が多すぎて何が自分にとって重要なのかわからない点', '今期の全体像の前半部分は、昨年度の講義資料があるためか、概要とは言え分かりやすくなっていた。', '今後の各回の授業で何を学ぶかについてのイントロダクションがあることで、今後学習するうえでの期待値があがったこと。', '各回の概要の説明があり、どのようなことを学んでいくのか見通しやすい点が特によかった。', 'LLMについてほとんど知らなかったので、それについての内容と全体像をざっくりと知ることができたことがよかった。特に、スケーリングのグラフなどは何度も説明があったので理解が深まった', '仕事上関わりが薄く単語だけ聞くことはあった内容が、今回概論として効率よく知ることができたのでありがたかったです。', '最初の話の内容が知らないことが多かったこともありますが、説明が早くて理解が追い付かなかないところがあった。再度見直して確認しようと思います。', '検索して固有の質問か判定したうえで、リアルタイムに質問するのは至難の業かと思いました。', '各授業の概要説明が今後授業を受けるにあたって有用であった．', 'Scaling Lawと呼ばれる経験則によって必要な資源の量を計算できるため，LLMに対する積極的な投資が行われているという話は興味深いと感じた．\\\\nまた，現在の日本のLLMに関する課題などは，日本の講義でしか知れないため興味深いと感じた．', '聞き取りやすいスピードだった．', '日本語LLMモデルに関する動向が興味深かったです。', '大まかな概要を伝えながら、少し具体的な事の紹介がありとても内容が充実していた。\\\\n具体的にはmc4などの普段聞きなれない単語が具体例としてスライドに書かれていると後から調べてみようと思うので、新たな知識を得るきっかけとなると思いよかったと思いました。\\\\n', '専門用語を軽く解説してから説明して欲しかった。', 'これから何を学ぶのかを理解できた。', '少しだけ，早口な部分を感じましたが，ボリュームを考慮すればそれほどでもない程度です．', '具体的な例を用いた説明が良かった。', 'ポインターを見逃してしまうことがあるので、赤い点などのものだとありがたい。', '2回目以降の講座の概要を1時間弱で説明頂いたことで全体の流れが分かり、各授業に臨みやすくなったかと思います', 'LLMを用いて、ロボットの行動系列を生成するという応用事例はとても興味深かった。今後も様々な応用例を自分でも調べて後学に活かしたい。', 'LLMについて全くの初心者だったので、LLMとはどのようなものか、なぜ注目を浴びているのか、簡単な原理を説明があったことで理解に役立った。', '初学者でもわかりやすく説明がなされていたこと。', 'ラグやMoEなど気になる技術もあり、今後学習していく内容がわかってよかった。', '今回の講義の全体像がわかったこと。', 'スライドの分量が多すぎるかもしれません。', '詳しい内容には触れていないが，大規模言語モデル初学者の自分にとってはよい1回目の講義だった．基本的な大規模言語モデルの考え方だったりをざっくり知れて，これからの学習のビジョンが講義前と比べて鮮明になった．', 'これから始まる講義の全体的な流れをつかむことができたことがよかったです。LLMについては対象事前知識を持っていたものの、その知識の確認としても本講義は役割を果たしていたように思います。', 'これからの講義でどのようなことを学習していくのか説明するのはいいのですが、すべての講義ごとにその内容を説明する必要はないのかなと思いました。', '次回以降の内容を細かく言ってくれるのは余裕のない身としては助かった。', '小規模言語モデルも時系列で大規模言語モデルの紹介に載せておいてくれると助かったかも。', '基礎的なことからLLM周辺まで、丁寧にカバーしてくださっているところがいいです。', '初学向けに、各回で学ぶ内容から、予習しておいた方がいい統計的、数学的知識なども紹介していただけると、予習に役立てることができ良いと思った。', 'RAGや、RLHFなどの説明の時、具体例をしっかり交えて説明した後に、一般的な説明をしてくださったので、すんなり理解することができました。また、図や表が適切な量で、見やすいと感じました。また、スケーリング則のグラフがたくさん出てきたので、講義を受け終わった後も、とても印象的なものとなりました。', 'LLMの概要を知ることができ、これから学ぶ分野への興味がわきました。', '後半の今後の講座の説明が詳細すぎ、記憶に残らなかったため、よりシンプルにご説明いただく方が今後の講義の流れを把握しやすいのではないでしょうか。', '最新のLLM情勢について学ぶことができ、良い経験になりました。', '声が聞き取りやすく、スムーズに学習できました。', '最終のコンペの成績次第では共同研究や他の処遇もあるとのことでしたが、具体的にスコアや実力がどれだけ必要になるのか、また、講義で出される課題以外で他にアピールする方法等がもしあれば、教えていただけたら幸いです。', 'スライドの流れがわかりやすく、今までのモデルとの比較がされていたので、LLMの立ち位置がよくわかりました。', '丁寧に説明していただき、とてもわかりやすかったです。', '学習に必要なだいたいのデータ量の話．', '第2回（9/11）前の事前準備の説明をしてほしかった．', 'わかりやすい講義ありがとうございます。', '講義の全体像を初回ガイダンスで攫うのは全体像が把握できて良いです', '字幕などがあればもう少しわかりやすかったです\\\\n', '体系的にまとまっておりわかりやすかったです\\\\n', '専門分野に特化したLLMの場合、ハルシネーションがどれぐらい改善されているかを知りたい', '全体像を把握できたのは良かったと思う、Robot Transfomerや日本での動向など個人では追いきれていないトピックがあったのでとても参考になりました。', '各回の概要を説明していただいたことにより、これからのカリキュラムのイメージができて講義に取り組みやすくなった。', 'LLMのパラメータ数の変遷やスケーリング則の詳細などは知らなかったのでよかった。', '説明の速度や声の大きさなどが適切で、聞いているのと同時に考える余裕も持ててよかった。', '受講者のレベルが高そうに見受けられ、講義についていけるか不安もありましたが、解説が非常に丁寧で安心しました。', '次回以降利用してみたいと思います。', '時系列に沿った言語モデルの進化と、直近の状況について、また日本におけるLLMの環境について学ぶことができた点がよかった', 'パラメーター、トークン、Iaas、などの基本的な用語の知識が不足していたので個人学習でカバーしたい。', '各回の学習内容について分かりやすい説明があったため、今後のモチベーションに繋がりました。', '特になし。', '周辺知識を交えながら講義をいただき勉強になった。', 'ありがとうございました。', 'この回答がどのように利用されているのか、👎を押すとどのようなワークフローで運営者さんに通知がいき、返信が来るのか、ブラックボックス化されている点が多いため質問やフィードバックのハードルが少し高かった。', '前提知識の少ない自分であってもアーカイブを見直すことで、何度も内容の確認をすることができました。', '今後の講義の内容を教えていただくことで、興味深い観点や疑問がいくつもあり、それが解決するのが楽しみです。最も興味深いのは、RLHFにおけるreward modelはどのように開発しているのか？についてです。最新のAIモデルを開発する際に、それの学習元となるモデルを先に開発する必要があるのか？というのが少し想像がつかない部分ではあります。学習が楽しみです。', 'LLMについての知識がほぼない状態で今回の講義に臨んだので、LLMとはなにか、なにに活用できるのかなど基礎の部分から知ることができたのがよかった。また、講座各回の概要について知ることができ、これからなにを学んでいくのかをつかめたのがよかった。', '実際に学習するにあたって必要となる計算資源、必要データなどについてはこれまで踏み込む機会が無かったため、現時点での日本におけるデータや資源について学ぶことができ、非常に勉強になりました', '全講義の概要を説明していただいた点\\\\nどんなことを学べるのか、何が身につくのかを想像してわくわくできました。', '今のところは特になし', '大規模言語モデルの活用例がおもしろかった。', '前半部分がすこし早口だった。', '予習テキストとの比較で、1年間のUpdateについて知ることができた。', '特になし', '特になし', '開発までできるGPU環境はとっても素晴らしいと思いました。', '全体的にとても丁寧に説明していただいたので、分かりやすかったです。\\\\n予習復習をして最後まで受講できるように頑張ります。', '貴重な時間をありがとうございました。\\\\n引き続き、よろしくお願いします。', 'ありがとうございました。2回目以降も楽しみにしております。', '各回の概要を簡単に押さえられること。', 'たまに質問の復唱のような内容になっていることがある...?でもかなりいいものだと思います。参考文献があって安心感があります。', '特に、ドメイン特化型のLLMが今後紹介されるとのことで楽しみです。', 'ありがとうございました！楽しかったです！', 'ありがとうございました！', 'シンプルにLLMとして性能が高いように感じ驚きました。おそらく参照する外部DBを変えているのかと思いますが、講義回や範囲内・範囲外を選択することで何が変わるのかGUI的に説明があればわかりやすいと感じました。また、slackの会話や操作感からWebブラウザのサービスとして少し不安定な感覚もあります。', 'これからの講義でフォーカスするところを丁寧に紹介してくれたこと。', '様々な理解度の方が参加されていると思うので、全体感をしっかり伝えている今回は重要だと思いました。', 'LLMとはどんなものか、また今後どのような学習をするか見取り図を得られた。', 'コンパクトにまとまって説明してくれてよかった。', '【必須】本日の講義で学んだことを50文字以上で入力してください。とありますが、可能であれば入力文字数が分かるように改修いただければと思います。', 'LLMモデルの原理において、直感的にわかりやすく数式を扱っていたため、基礎的な確率論などの数学の知識がある私にとって理解しやすい内容であった点', '後半回は不確定な部分はあるものの、各回にどのようなことをするのか説明があり、イメージが湧きました。', '概要として幅広い内容を聞けて良かったです。', '今後利用して改善点があれば記載させていただきます。', '去年の内容をアップデートされていることや新しい項目の追加を事前にお伝えいただき、わくわくしました。', '2024で大きく更新される内容を学べることがわかったこと', '講義関係の情報をもう少しRAGの範囲に加えていただけると幸いです', '特に良かったことは各回での勉強内容を端的にわかりやすく説明しており、各論へ楽しみが出来た。', '特になし', '話し方、内容ともにとても素敵でした。', '昨年に続いての受講です。前回の内容との比較でどのようなアップデートがあったのかが簡単に説明されたのが良かった', '半導体に関する話は今まであまり意識していなかったので、どういった内容なのか想像できなかったが、とても楽しみです。', '今年もよろしくお願いします！', '承知しました。', 'イントロダクションという位置づけとしてとらえて、全部理解しようとするのではなく肩の力を抜いて参加しました。今後の講義の見取り図になっていると思います', 'AIチップの話が印象に残った。高性能化と小型化が進めばLLMの活用事例はさらに広がると思った。\\\\n', '「製造LLMについて教えて」を質問したところLLMの製造方法について回答された。期待する回答は製造業に特化したLLMについてだった。(ChatGPT 4oは期待通りの文脈で回答が返ってきた)', '前回との差分の講義が半分を占めており、以前の講義のアップグレードもあるとのことで、非常に期待を持てた。感謝です。', '時間に余裕をもってZoomに入室しようとしたが、直前まで開かず不安になった。', 'まともな質問をする人はチャットbotで質問し、ダメな質問をする人はチャットbotを使わずSlack上でいきなり運営に質問を飛ばしてくるはずなので、それを防ぐ方法が必要だと思いました。例えば１次回答をSlack上で自動返信して、運営に直接質問したい場合のアクションをするとスプレッドシート上に要回答リストとして起票される等。', 'LLMのこれまでの発展の歴史と現状，将来展望を大まかに理解できた．', '昨年にくらべてパワーアップしているのがわかって期待が持てました。', 'SlackにURL貼り付けが禁止とあったが、実際にはやってしまっていた人がいた。仕組みでリンクを排除できないか？', 'スムーズにいっていてよかった。', '昨年の受講者でも優秀生の候補としてほしい。', 'リストで、質問と回答を並べて表示して、いちいち開かないでいいようにしてほしい', '今年度のセミナーは昨年度版よりだいぶアップデートされる予定ということを伺い、かつ実用寄りのトピックが大幅に増えるとのことで、大きく期待を持っています！', '初回のオーバービューということで仕方ないと思うのですが、既習者向けには若干冗長に感じました。', 'お二人とも聞きやすいトーン・スピードで喋っていただき、大満足です！', '講義回数も増えて若干不安ですが、受講頑張ります。それにしても、本セミナーを通じて4000人もLLM人材が増えたら凄いですね...。', '申し訳ございません。まだ利用できていないため、今後使用しFBできるようにいたします。', '昨年度からバージョンアップが随所に散りばめられていたところ', '特に無し', '特に無し(満足)', '今年も楽しみにしています！', '昨年度の Day1 内容との差分（「大規模言語モデルの進展」「領域特化LLM」「ロボット応答／行動系列生成や画像生成」「GENIAC関連の概要」「日本語LLM関連情報の進展」等）を聞けたことが、特に有意義でした。', 'まだ使っていません。', '後半における第二回プロンプトの説明の中で step by step をプロンプトに入れればとにかく性能が上がると聞いてよく使っていたが、なぜ才能が上がるのか理解できた', 'slackで質問するよりは大幅に聞きやすい環境になったと感じた', '上述つの通り、\\\\n世間では、生成AIがさも凄いものであることが褒め称えられているが、日本語での利用に関しては、まだスタートを切ったばかりだということが理解できた。', '意図的かもしれないが、どんなモデルがバックエンドで利用されているのか知りたいと思った', 'Transformer→言語モデル（FineTuningなど）→基盤モデル（promtなど）の発展の遷移を理解できた。', 'きっかり、時間通りに講義を終えたところ。素晴らしです。', 'こんかいチャットbotを使う機会がありませんでした。', 'スケールについては、どうしても対数表記でわかりにくい部分があったので、海外との規模間の違いがわからなかったです。', 'とっつきにくいと思われるLLMについて、わかりやすいようにポイントを絞って説明していただいたのが良かったです。', '文献を表示してもらうときは3点併記される形で表記されると思います。それぞれどのような特徴があるのかを示していただけるとありがたいです。', '今後の授業がとても楽しみになりました。', '特にありません。', '・後半の「各回の概要」がシンプルでわかり易かった。', '最初に全体像とその説明があったことで、どのように自分なりに学ぶべきか道しるべになり、不安を軽減して講義を受けさせていただけると感じました。', '恐縮ですが、1度聞いただけですぐに理解できなかった箇所の資料にさらに補足説明があるとうれしいなと感じました。\\\\n例えば、条件確率の数式の箇所。グラフなどの軸が何を意味しているのかなど。', '検索しても部分一致しなくて、検索にヒットしないことが多いです。', '事例に親しみやすさを感じました、吾輩は猫である。とか、言語モデルがたぬきというのが良かったです。', '特にありません。', 'ゆっくりと丁寧な言葉で専門性の高い話を聞かせて頂くことが出来ました。非常に興味深い内容でした。', '素晴らしい講義を受講させて頂くことが出来ました。有難うございます。中々ここまで深くLLMについて知る機会がありません。本当に効率よく専門的な内容の話を噛み砕いてご説明頂いていますこの講座は貴重な勉強となります。これからも宜しくお願い致します。', 'チャットbotに付きましては、まだ不勉強でよく分かっておりません。', '難しい内容でも、具体例をもとにわかりやすい説明をしてもらえて良かったです。', 'Slackのどこに必要な情報が記載されているのかわかりにくいです．', 'LLMの基礎知識を得ることができた。', 'LLMの応用についての説明が省略されていて、概要が分からなかった。', '後半のB先生の講義資料が、直前にアップされた資料と中身が少し異なっていたので戸惑った。', '講義を提供していただき、ありがとうございます。', 'まだ使用していません。', 'LLMの全体概要を把握できたところ。特に、LLMの学習において重要な、計算資源、データセット、大規模言語モデルなど整理ができた。', '特になし', 'もうちょっとマイクからの音声がクリアであるとよかった。※聞き取りには十分だったが、若干曇った感じになっていた。', 'ありがとうございました。引き続きよろしくお願いいたします。', '各回の関係性を明示しながら進めていただけた点．', '特になし', '特になし', '効率的に全体像を掴め、今後の見通しがたつ構成になっていたのは安心感があってよかった。', '概論なため仕方ないが、知っていることも多かった。', '初学者向けに気を配った構成でありがたかったです。', 'LLMの概要と今後の講義の流れがよく分かりました。特にRAGについては、LLMを活用した学際的な研究への展開が期待できると感じました。', 'MoEがしっかりと学べていなかったのでとても楽しみにしています。', '今後の概要について一通りしれた', 'Overviewと言いながらもこの先の各回の講義について内容をある程度触れてくれていたこと。', '今後，各回で学ぶLLMについて体系的・俯瞰的に理解することが出来た点が良かったです．', '今回は，LLMの概要を紹介する内容だったと思うので，その点でわかりにくかった点はありません．', 'LLMの最新の状況について，世間では様々な情報が飛び交う中，それらを整理して講義して下さった点が良かったです．', 'このような機会を社会人(公務員，研究者，スタートアップ社員)の方にも開講していただきありがとうございます。大変勉強になりますし，今後，自身の研究にも活かしていきたいと思います．', '今回は使っていませんでした．次回以降試してみたいと思います．', '・全体像が把握できた\\\\n・日本の最新状況（モデル／GPU環境／データ／評価）を知ることができた', '概要を明示していただけた点です。また、昨年からどのように変わったかを説明していただけたところが、よかったです。どのように技術が進展しているのかがわかるので、よかったです。', '今回の講義、ありがとうございました。また次回も楽しみにしております。', '動画に時間指定のチャプター（目次）を設定すれば見返すときに役立ちそうだと思いました。', 'これから開催される講義の概要を説明していただいたのは、よかった。\\\\nまた、資料に掲載されていた論文は読んでおこうと思いました。', '大規模言語モデルは消費電力量が莫大でトレーニングの際に必要だった冷却水の量は70万リットルにも上ったという記事も見たので、今後は環境への配慮も必要だと感じました。\\\\n米国とは違い限られたリソースで、消費電力は少なくても創発的能力の高いモデルをどうやって開発していくのかとても気になっています。', '回答するにあたっての参考文献が明記されていて、信憑性が高いと思いました。面白いので利用させていただきます。', 'Scaling Lawの発見がLLM開発の見通しを非常によくしたことがわかり、この数年の技術進歩がこのような知見に基づくものということが理解できました。', '試しにLLMの汎用性について説明してくださいと聞いたら、多様性の話になっていました。。。', '全講義の概論を述べてくださったのは全体像がつかめてよかったです。', '質問はチャットで出来た方が良き気がします。なお、録画だと倍速で観れるのは大変助かります。', '社会実装の例を技術的な発達とリンクしてサービス名などだけでも示してもらえると理解が進むと感じました。', 'slack慣れないといけないですが、難しいです。', '日本初のモデルの開発の進捗や海外のモデルの継続と事前学習からやっている違いを知れたことがよかったです', '本日は特にありません', '各講座で扱う要素について概要を先に教えていただけたのは、自分の学習ポイントを考える上で有用でした。', '特に無いです。', 'テンポよく教えていただきました。', 'これからよろしくお願いします。', 'UIの関係か、最初に検索欄に文章を入れて戸惑いました。また、チャットbotの過去の回答についても自然文で検索できるとなお便利です。', '今後も楽しみにしています！', '講義動画、見直します、、', '検索結果が20件程度しか表示されないので、探しづらいです。100件など多めに表示して頂けると(表示件数を指定できると)ありがたいです', '講義の全体像について事前に説明があり、非常に親切に感じました。事前に資料を見たときは、理解できるか正直不安でしたが、ひとつひとつ丁寧にご説明頂き、LLMについての理解が深まった気がします。', '学習におけるパラメータや計算量、データ、並列処理など理解が難しく、予習復習が必須と感じた。', '専門用語について、初学者や専門知識がなくてもなるべく理解できるよう噛み砕いて説明頂いてると感じた。', 'LLMの開発している方がよく「LOSS」という言葉を使っているのは知っていましたが、何を指しているのか知らなかったので今回説明があったので理解することができました', '業務だとLLMは言語に閉じてしか使用していないので、マルチモーダルの事例も知れたのがよかったです。特にロボットテックでの過去事例にも触れているのがこちらの講座ならではかなと思いました。今後の講義でも登場するのが楽しみです。', '不満点は全くなく、あくまでNice to Haveな観点です。Zoomの仕様にも制限されると思うので実現性は低いかと思いつつコメントします。録画を見返す際、資料の切り替え時のみではなく、トピックが切り替わった時にもタイムスタンプが残っていると振り返りやすいなと思いました。', '紹介いただいた事前学習用の日本語データに関して，その後のスライドで紹介された注意点との関係も説明していただけたらなおよかった．例えば学習データのライセンスはどうなっているのかや個人情報の有無について．', 'よくまとまっていた点\\\\nアーカイブ視聴可能な点\\\\n最新動向がわかった点', '特にありません', 'すごくよかったです\\\\n今後の講義が楽しみになりました', '導入ということもあり、難易度が優しかった。', 'LLMについて、日本全体のレベル底上げを熱く取り組んでいるということが伝わってきました', '講義全体の説明があることで、今後に期待できるところ。', '以前と同じタイトルでもアップデートがなされているとしれた点\\\\n', '特にありません', '特にありません', '２回目も色々と学習させていただきます。楽しみにしています。', 'ひとまずこの環境を使っていきたいと思いました。', '「価値観」の部分ですが、究極のことを考えてしまうと、どうしてもこの部分にセキュリティ（？）や倫理性のことなどが気になってしまいます。\\\\n今は生成AIの開発など、どんどん進めていくべきということは理解できますが、一方で、人間としての（もしくは人類として）価値判断基準というものを見失ってしまわないかという危惧はどうしても残ります。\\\\nすみません、余計なコメントと思いつつ、記述させていただきました。', '全体の概要を簡潔に伝えていたのが素晴らしいと思った', '事前学習と事後学習が混ざった知識になっていたので、明確に違うことがわかったこと', '特にありませんが、受講生の特徴や人数などもう少し受けている人の特徴を知りたかったです。', '講師は、第一線の研究者の方ですが、説明がお上手過ぎて驚きました。ありがとうございました。\\\\n色々なLLMについての資料を見てきましたが、体系的にというより理論的にまとまっている最高の内容だと思います。', '参加させていただきありがとうございます。\\\\n', 'すいません。まだ使っておりません。', '前半の講義では、LLMをいま学ぶ理由を端的にまとめてくださり、私の何となく考えていた内容を根拠を持って示していただき、嬉しかった。\\\\n後半では、ダイジェスト的にこの講義全体をまとめてくださっていた点。今後学ぶことがクリアになったと同時に期待が持てた。', '聞いたり触れたりしていた生成AIの各トピックについて概括的な理解（全体像）を得ることが出来た。\\\\n特に生成AIの活用について半導体がどのように関係するのか第６回の講義が非常に楽しみになった。', '特にありません。', '各回のトピックがどういった意味合い・なぜ重要な位置付けを持つのか概要が分かり、関心が高まりました\\\\n', 'これだけの人数・高い質の講座の運営を頂きありがとうございます。\\\\nしっかり学ばせて活用させてもらいます。', '言語以外のモダリティにもスケール則が成立すると初めて知ることができた。今後言語以外のモダリティの性能も言語と同じぐらい上がっていくと考えるとわくわくする。', 'OllamaでTanku-8bインストールできました！\\\\nhttps://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF\\\\nモデルをDownloadする\\\\ncd ~/Donwload\\\\n\\\\ncurl -LO https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF/resolve/main/Tanuki-8B-dpo-v1.0-4k-Q5_K_M.gguf\\\\nImport用ファイルを作成する\\\\ntouch ./Modelfile\\\\nVimでフィアルを開いてコピペする\\\\n```\\\\nFROM ./Tanuki-8B-dpo-v1.0-4k-Q5_K_M.gguf\\\\nTEMPLATE \\\\\\\\\\\\{{ if .System }}<|start_header_id|>system<|end_header_id|>\\\\n\\\\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\\\\n\\\\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n{{ .Response }}<|eot_id|>\\\\\\\\\\\\\\\\nPARAMETER stop \\\\<|start_header_id|>\\\\\\\\nPARAMETER stop \\\\<|end_header_id|>\\\\\\\\nPARAMETER stop \\\\<|eot_id|>\\\\\\\\nPARAMETER stop \\\\<|reserved_special_token\\\\\\\\n```\\\\nインストールする\\\\nollama create llama-tanuki-8b -f Modelfile\\\\nollama run llama-tanuki-8b', '・LLMのざっくりとした技術概略、昨今の情勢等、上位の情報がしっかり整理されていて良かった\\\\n・エビデンスや裏付けデータを交えながら、かみ砕いて説明いただいて理解しやすかった', '・LLMの講座各回概要も表面的すぎず、過度に詳細な内容でもなく、良いバランスで次回以降へのつながり・期待を抱くことができた。\\\\n・程よいインターバルで講師がバトンタッチしていき、雰囲気が変わって良かった。', '毎週水曜が楽しみになりそうな、とても良いスタートでした。\\\\nP.S. 超高速の学習可能なAIチップが巨大すぎて驚きました。', '理路整然としており、大変素晴らしかったです。ありがとうございます。', 'LLMに関する重要概念や全体像を分かりやすく教えていただけたこと。', '膨大な情報の中でエッセンスとなる部分と、それらの情報のインデックスが適切に作成されており、理解がしやすかった。また、自分で深めるための足掛かりとなる情報を残していただけているので、探索もしやすくなった。またこれからの講義の全貌が見えて本当に楽しみになった。', '最高です！ありがとうございます！！！', '特に前半の講義で，言語モデルの基礎理論と大きな流れを理解することができたのはよかったです。掻い摘んだ一部の知識しかありませんでしたので，参考になりました。', 'わかりにくかった or 改善点というわけではないですが，ロボティクスの応用まわり（言語以外のドメインの話）の詳細は可能であればもう少し理解したかったです。', '時間配分もよく，大変わかりやすいお話でした。', 'ありがとうございました。', 'わかりやすくシンプルな説明でありがたかったです。完全文系ですので、ドキドキしておりましたががんばっていけそうです。', 'LLMの開発ステップがわかったこと。\\\\n以前、松尾研GENIACの表彰式ライブを視聴した時に出てきた、わからない用語が少しわかったこと。', 'お二人とも、時間配分もよく、概要をわかりやすく説明してくださり、良かったです。', '全体的に概要が分かりやすかったです。特に、講座各回の概要は文章だけでなく図があってイメージしやすかったです。また、日本を取り巻く概況が整理されていて分かりやすかったです。', '説明がスムーズで理解しやすかったです。', '今後のセッションの内容をかなり詳しく、具体的に説明していただけたことが大変よかったです。', '大変分かりやすかったです。', '予告の内容が詳しかったので今後学ぶ全体像が把握できた。', '講義の全体像や流れを説明いただけたので、学ぶことのイメージがつきやすかったし、事前に講義内容を簡潔に説明いただいたことでこの回ではこのような内容が学べる、理解できるのかと楽しみに感じた。', '具体的でわかりやすい資料でした。', 'スピーチのペースが丁度よく、とても聞きやすかったです。', 'このような充実した講義を無料で受けられることに感謝いたします。', '講義の全体像を、かなり分かりやすく教えていただけて、講義のスコープを把握することができました。', '言語モデルのロジックや計算方法、その計算方法は数学的にどの分野のものが使われているかという説明が良かったです。', '今後、LLMやアルゴリズムを開発するにあたって、数学や物理学など具体的に何を勉強すれば良さそうか教えていただけると嬉しいです。', '全体の概要が丁寧にされていることは良かった。', '\\\\Emergent Ability\\\\などの知らなかった専門用語のキャッチアップができて良かった．', '各講義の概要を段階的に要約して説明頂けてよかったです。\\\\n今回の講座で学ぶべきもののイメージが凄くつきました。', 'GPUの性能が近年急激に上がっていることが衝撃でした。\\\\nGPU資源に関して海外より遅れていると思っていましたが、\\\\n後継世代の性能見ると確かに日本にも利があるなと思いました。\\\\n\\\\n現在、RAGやファインチューニングどちらで社内データの活用していけばいいのか悩んでいたりします。\\\\n本講座を通してまずは基礎的な部分を理解してそれぞれの利点などを学んでいきたいと思います。', '言語モデルの成り立ちからや歴史まで丁寧に解説していただき、この先の講義が楽しみになりました。\\\\n特に、LLMが長いコンテキストを処理できるようになった仕組みが気になりました。', '今理解しなくてもいいところが明確あったこと。全体像がつかめて、今後の学習意欲向上につながりました。', '各回の説明により、今後の流れを理解できた。\\\\n日本版LLMについて説明があり、今後の進展に期待が持てた。\\\\n', 'とてもわかりやすい説明だった。', '難しい内容をわかりやすく解説してくれている点。', '詳細な最新動向や最新モデルに関する説明', '本講座に於ける注視すべき論文などを引用する形で、学ばせたいことを分かりやすく説明した点が良かったと思う。', 'B先生の、日本語モデルの評価方法のまとめ資料、LLMを学習させる場合の計算リソースの概算資料は、とても役に立ちます。\\\\n', '若い方々への教育が目的とのことですが、社会人や他分野の人たちの本講義のかかわり方について、都度示唆頂けると助かります。\\\\n', 'LLMについて知らないことが多く、丁寧に言葉を選びながらゆっくり話していただいたこと感謝いたします。とても楽しく勉強でき、2時間があっという間でした。事前に勉強した時にスライドの絵が難しくてドキドキしていましたが、分かりやすかったです。小児科医として、これからの医療AIに役立てるために、復習、予習を繰り返します。', 'とてもよかったです🎵', '感謝しかありません。有難うございました。', '次回もワクワク、ドキドキ', '使わなかったので、次回試してみます。', '非常に体系だった講義で講師の方もわかりやすい説明で非常に良かったです。\\\\n今後の各回が今から楽しみです。', '今後の講義概要について事前に把握することで、学習のロードマップを知ることができた点が良かったです。説明の中で気になる点もあったため、今後の講義や講義外の学びの中でそれらを理解していくモチベーションが湧きました。', '特にありません。', 'これからの講義を楽しみにしています！', '後半で、次回以降の学ぶ分野と概略をお話いただいて、予習をしやすくしていただいた点。', 'どの講師の方もゆっくり丁寧に話していただいて分かりやすかったです。', '次回以降もよろしくお願いいたします。', '今回はございません。', '最新のLLM事情、特に日本の現状がよくわかりました．', 'お二人とも話し方（速度、声の大きさ）など適切だったと思います．', '今回はガイダンスが中心でしたが、全体の説明からも熱意のようなものが伝わってきました．今後のやる気を上げる内容でしたので、それに応えるよう学んでいきたいと思います．受講者が4000人と聞いてちょっとびっくりしました．', 'まだ使っていないです．試してみます', '昨今のLLM概況を短時間で俯瞰できた事に加え、本講座後半にはロボテクスなど具体的な活用シーンでLLMを学べると期待が持てた事', '同志達の熱気やリアルタイムなライブ感がZOOMを通してあまり感じなかった', '予定時間内でまとめられていて素晴らしい時間管理でした。', 'まずは概要から入ったのは良かったと思います。', '特になし。', 'これが無料はすごいです！\\\\nめっちゃ勉強して仕事でLLM活用します！', 'chatbotに通報ボタンが付いてて、変な質問したら通報されると思うとちょっと怖くて質問ハードル高かったのでもう少し怖くないようにしてほしい。', 'LLMと関連分野は進歩が速いため、昨今の状況をまとめて聞けたのがよかった。', '事後学習とRLHFの違い。なぜこの2つを分けて実施しなければならないのか必然性が理解できなかった。', '計算回数を見積もる式に関連して占有されるメモリ量の見積もり方も知りたいです。', '投稿した内容が共有される範囲が気になりました。', 'LLM以前ではモデルごとに学習していたのに対して、LLMでは大規模なテキストデータから事前学習をして汎用モデルを構築して、目的のモデルにファインチューニングしていくことがわかりよかったです。概要だけではありますが、ファインチューニングが人の手によるものなのか、気になりきいていました。また学習に用いるテキストデータの上限に達した後、どうするのか（音声データのテキスト化？）など今後の講義がとても楽しみになる一日目でした。', '全体的', 'いまのところはよさそうです。', '今後の各回の内容のさわりを第一回で説明してくれたのは大変良かったです。', '元が英語が多いのは当然ですが、NNとか元の英語を書いておいてくれた方が良いように思います。世界と伍して行くためにも、教材の半分ぐらいは英語でも良いかもしれません。', '特に不満は感じませんでした。', 'まだ使っておりませんので要望はありません。', '本講座の全体像だけでなく、LLMの概要についてダイジェスト的に概覧できたのがよかったです。', '今のところありません。', '今のところありません。', '本アンケートフォームの【必須】項目と（任意）が黒文字なので、【必須】項目のみ、赤文字などにすると視覚的にわかりやすいと感じましたた（講義について非常にわかりやすかったです！）', '非常にわかりやすく、有意義な時間を過ごせました。', '谷と申します。この度は社会人枠での参加となりますが、貴重な機会をいただき感謝申し上げます。全12回の講義を受講し、業務等で役立てられるよう残りの講義やコンペも頑張ります。', '各週で、この部分のところのことについて学びますというお話をしていただき、すごくわかりやすかったです。', '今の段階では、まだ分かりにくいと思うことが分からない状態です。これから学んでいきたいです。', 'とても分かりやすく、噛み砕いて話をしていただいたと思います。ありがとうございました。すごく分かりやすく、これから頑張ろうと思えました。', 'まだまだ知らないことだらけですが、一歩一歩積み重ねていきたいと思います。頑張ります。', 'B先生の講義は、聞きやすく、わかりやすかった。', 'GPT3, 4などに対して具体的な学習のための計算量について書いてあるなど、具体性がある内容のほうが個人的に興味を持つことができるため、そういった記述が多いのがよかったです。また、最新のモデルや内容を反映させているところがよいと思いました。', '講義資料にリファレンスがついていたため、復習がしやすいと感じ、良いと思いました。', '日本の生成AIの現状について明るくないのではと感じておりましたが、今回の講義で、後追いのメリット（よりデータを載せられる最新のGPUを使用できる）ことや欧米と比べ、利用できる学習データの自由度が高いこと、日本人らしい文章の作成は、日本産の生成AIが得意などを伺い、むしろ今後日本の生成AIがどのような発展するのかが楽しみになりました。', '今後の講座内容を事前に知ることができたため、前提知識を持って挑むことができる。', 'わかりやすく説明いただけました。', '途中にお手洗い休憩がほしかった', '最近の動向などを紹介してくれた点。', '各回の概要を教えていただき、今後どのような内容を学ぶことができるかのイメージがわきました。', '特にありませんが、各回の課題（宿題）がどのようなものになるのか少し不安です。', '非常に詳しく丁寧にご説明いただき、初見な内容も理解できました。', 'これからの講座も楽しみにしています。ありがとうございました。', 'RAGと事後学習の違いがよくわからなかった。RAGを構築するくらいなら、事後学習してしまってよいのではないか？', '・初回の講義で今後の講義で学ぶことの概要を俯瞰的に学べたことで、次回以降もゴールを見据えた目的意識を持って学習できそうと思えた\\\\n・LLMやそれを取り巻く環境、および関連技術の概要を学ぶことができて、LLMについて大まかな理解ができた', '・特にありません', '・どの講師の方も明瞭にお話しをされていて理解しやすかったです', '・初回の講義を受けてLLMに対するイメージが今までよりクリアになりました。今後の講義も楽しみにしております。', '最近のLLMや新しいものを知ることができたのが良かった。', '今後学ぶ事柄が詳しく解説されていて分かりやすかった。\\\\n話すスピードがちょうど良かった。', 'LLMの基本から応用までの全体像を理解できた', 'LLM以前に、そもそもAIとはといった部分もスライド1枚程度で簡単に説明いただけるとよかった。', '講義内容の質問と、講義運営の質問、その他の関係のない質問、雑談といった質問が分類されると考えられるので、講義内容の質問とそれ以外の質問が明確に分けられるようにできるとよい', '社会人として実務でLLMアプリ開発等行っているため、昨年の内容から新しく追加された安全性と、ドメイン特化のLLMの講義を特に楽しみにしています。', '講義は大変分かりやすかったです。また昨年の資料なども事前に格納頂き、予習がしやすく助かりました。', '前半のA先生のLanguage modelの歴史的な話でどういう弱点があったのかと言うのを改めて頭の中で整理できた。\\\\n\\\\n', '後半に関しては、このコースで学ぶことのOverviewを把握できるのは良いのだが、ここの1時間かけるのは少し冗長かなと思った。\\\\n前半のA先生が話されたTransformer以前の歴史的な経緯を充実させた方が、重複が減るかなと思った。', '次回からの講義が楽しみです。', '非常に広い観点から概観することができた。', 'LLMをファインチューニングして自分なりのモデルを作れることを知り、ビジネスへの応用の可能性を感じた。', '特にありません。', '難しい内容を１時間という制約の中で、澱むことなく、また、シンプルに説明されており、大変良かった。', 'ご自分の研究もある中、このように世間一般の人々に時間を割いていただきありがたく思います。今手掛けている仕事にも応用できるよう授業を活かしたいと思います。', '使ったことがないので、今後使ってみて気づいた点があったらコメントします。', '網羅的な解説を2時間に凝縮してもらうことで、頭名の中での体系的な状況理解が促進されたと思う。', '今回わかりにくかった部分は今後の講義で改善していくと思っている。', '声の大きさや話のテンポなど全体的に満足。', 'まだ多用しきれておらず、今のところなし。', '上記です。', '重要なポイントについては繰り返し説明してもらえると嬉しいです。', 'とてもおちついていてよかったです。', '貴重な機会ありがとうございます！引き続きよろしくお願いいたします。', 'わかりました。今後FBさせていただきます。', '日本の技術レベルが世界的に見てどの位置にあるのかを、パラメータ数を使って示している点がわかりやすくて良かったです。また、日本のLLM開発を牽引している企業がまとめられており、参考になりました。', '昨年はこのような講座が実施されていることを知らず受講できませんでしたが、年明けにSNSで講義資料を拝見してから、次に同じような機会があるならぜひ受講したいと考えていました。皆様のご尽力により、社会人の枠を広く設けていただけて大変感謝しております。12月末までの期間、どうぞよろしくお願いいたします。\\\\n\\\\nこの講座では間に合わないかも知れませんが、修了証をPDFでいただく代わりに、オープンバッジをいただけると嬉しいなと思いました。', '2回目以降の講義で学ぶ内容の概要紹介が良かった。(来年度以降の提案として) 概要紹介は第0回として一般に公開し、大規模言語モデル2024のコースに受講登録する動機づけとしてもいいのではと思った。', 'LLMに至るまでの深層学習の簡単な歴史から、現在のLLMの仕組み、日本のLLMを取り巻く環境、LLMを作るために何が必要かなど、LLMに関する全体像を把握できた。', '説明はわかりやすく講義自体はとてもよかったのですが、講義中に Slack の通知があると気が散ってしまうので、できれば講義中のメンションは少なくしてもらえると幸いです。。。', '仕方ないことかもしれませんが、やはりチャットボットとなると色々試したくなってしまう人が多いようで、重要な質問が埋もれてしまうのが少し気になります。チャットボットそのものに対する改善要望というよりかはUI/UX面に関する意見になりますが、例えばQ&Aごとに「いいね」などをつけ、いいね順で表示するなどの機能があると、重要な質問が浮上しやすくなるかと思いました。', '全体のアジェンダの提示', 'ファインチューニングやRLHFなどについて学べたのが良かったと思います。', '忘れていただけかもしれないが、Pre-train, Prompt, Predictという括りでの解説は新鮮だった。', '具体的にコード読んでなかったが、事前学習を誤解していたかもしれない。教師なし学習と認識していたので、正解ラベルに対してのロスを ... というので引っかかった。次回の講義で再学習したい。すでにやり尽くされていると思うがモデル並列でテンソル分解というのであれば、様々な行列演算や正則化などがあり得るのかと思った。', '申し訳ないのですが、どこにチャットbotがあるというのは知ってましたが、どこにあるのかわかりませんでした。', 'Zoomのチャット欄にリンクが設置してあるとありがたいです', 'このシリーズの全体像を総覧できた。断片的に見聞きしたことが、よくまとまっていた。', '大規模な学習環境、コミュニティーが構築されていることに感銘を受けました。これからよろしくお願いいたします。', '３人が、それぞれ得意分野を分担してお話しいただけ、現実味があった。', '専門用語、固有名詞のURL付きの一覧があると嬉しい。', 'それぞれの日の要点がよくわかった。最終課題は、前回の例を具体的に示してもらえるとよかった。講義の内容が追加担った部分は最終課題にも反映して追加になることは想像できる。', '学会発表する際に、講座の内容をどの程度まで参照してもいいか、過去事例を見て考えます。\\\\nアンケート回答はチャットbotで作成可でしょうか。', '各回で何をやるかを説明いただいた部分。', '全体の講義内容のつながりがよく理解でき、学習のモチベーションが上がりました。', '松尾研でされている取り組みも紹介されていたこと。', 'bot自体に問題は感じていませんが、可能であれば問題点とそれに対する解決方法を共有してほしいです。', 'LLMの大まかな仕組みと各回の概要の対応関係をわかりやすく説明していただき不安が取り除かれ期待感が高まった。', '体系的なまとめていただいたことで自分の中の理解が整理された点', '不明点はございません', 'みなさま、丁寧な説明で聞き取りやすくとてもありがたかったです。\\\\nありがとうございます。', '講義中にみなさまが使っているのを見させていただきました。一覧画面にて、同様の質問をまとめる、タグをつけるなどできるとストック情報になるのではと思いました。', '検索困難。', 'もし可能でしたら、コンペの内容を早い段階で共有をお願いしたいです。（実際に時間内にGoogle Colab等の環境構築？対応できるか不安なためになります）', '個人的には画像の分野と言語の分野で、それぞれを技術的に転用・流用したり、流行り廃りもあったりする印象だったので\\\\nその代表的な技術の変遷が時系列などで改めて理解できるとより理解が深まりそうだなと感じました。', '発話にフィラーもなく、淡々とお話されていた印象でしたが\\\\n内容も聞き取りやすく、分かりやすかったです。ありがとうございました。', '講座全体の流れが把握できたところ。', '分かりやすく図解してくださり、理解が深まりました。特にRAGに関する説明はとても分かりやすかったです。', '第5回の概要のところで、行列に関する説明の箇所が分かりにくかったです。', '全体的に分かりやすく説明してくださり、LLM全般に関してと、今後の講義の流れについての理解が深まりました。', 'とても楽しみにしていた講義に参加することができて、LLM全般の理解が深まり、今後の講義に対してのモチベーションが益々高まってきました。次回以降の講義も楽しみにしております。宜しくお願い致します。', '今後使っていきたいと考えております', '特に後半で、アップされている講義資料には含まれていないスライドが実際の講義では何枚か加えられているようですが、今後講義資料は更新されるでしょうか。', '参考資料が十分に紹介されていたので、勉強しておきたい。', '概要だけだったの各種アルゴリズムの動き把握できなかった。今後の会で紹介されると期待している。', 'みなさんわかりやすくて良かったです。', '楽しかったです。また次回お願いします。最後のコンペ楽しみにしてます。', '今後の講義内容が理解できた。\\\\n今後の講義においてどういったモチベーション（予習）で受ければよいのか知ることができ、よかったです。', '最後の方が時間が押していた為か、まとめてざっくりしていた印象でした。', '今後の講義予定が事務的ではなくそれぞれの内容について具体的なお話で今後が楽しみになりました', '専門に勉強していない社会人にもわかるようにかみ砕いて説明頂いていて大変助かりました', '概要なので広く様々なデータがかいつまんで説明されていたので分かりやすかった', '説明の速さが適切であった', '本日はありがとうございます\\\\n修了認定目指してがんばりますのでよろしくお願いいたします！', '単にLLMを使っている側の人間だったからか知らなかった、LLMの学習ステップについてが新鮮だった。\\\\nまた、LLMがGPTであることを再確認させてもらい、改めて考えさせられた。\\\\nそして、LLM の MoEについて、自分が誤解していた可能性を認識させてもらった。', 'Flamingoなどの他ドメインと組み合わせた手法や、LLMのロボットへの活用方法、スケーリング則の計算式や効率的なファインチューニングの方法、人による評価を行うことで強化学習でモデルを改善する方法、半導体についてなど、日ごろ気になっていた内容について具体的に学ぶことができたのが大変良かったです。\\\\n社内用のLLMへの新たな機能追加やファインチューニングなど、実務に活かせそうな内容が多いです。', '大変広範で深い知識をもとに、今回1回だけでも様々な知識をご教示いただけたのが良かったです。\\\\nまた、最初の回で、今後どのようなことを学ぶのかの概要を知ることができたので、具体的なイメージが付きやすく、意欲もわきました。', '大変勉強になりました。ちょうど実務で活かしたいものの、自分ではどのようにすればよいか分からず困っていたことについて詳しく学べそうなので、とても助かります。\\\\nできればグラフニューラルネットワークについては是非単体で詳しい講座を出していただきたいです！', '通報や検索の仕組みまで実装されているのがすごいと思いました。利用回数上限や利用上の注意点が分からなかったので、分かりやすい場所で周知されているとよりよくなると思いました。', '特に分かりにくい点はなかったです。', '情報ソースとなる論文紹介があり、講義部分の知識の掘り下げにつなげられると感じました。', '特に感じませんでした。', 'まだover viewですが、今後どんなことが学べるのか、どうそれを活かしていけるのかを考えると心躍りました。これからどんどん理解するために、予習・復習が必要となってくるとは思いますが、より多くを学び、多くのことを吸収して自分の今後の活動や夢に活用できるようにしたいと、改めて感じました。', 'また使用しましたらFBさせていただきます。', 'LLMの開発に言語学の知見は不要でしょうか?', 'slackで運営の情報発信が多く、どれが重要な情報なのかわからない。', 'ぜひ使いたい。', 'とても分かりやすい講義でした。ペースも丁度よかったです。', '前半資料のP22「Pre-train, Prompt, Predict」\\\\n今後のLLM利用は、如何に汎用モデルをうまく活用できるかが重要であると理解した。', '個人的には、第2回：PromptingとRAG、第3回：Pre-trainingに期待しています。\\\\n特にTransformerについての疑問を解決できればと思っています。\\\\n・単語のベクトル化はWord2Vecを使用している？学習済みでモデルを利用している？\\\\n・Attention機構は内積計算で単語の特徴量を最適化させる計算と理解、\\\\n\\u3000どの部分がニューラルネットワークでどのように学習しているのか不明\\\\n・各々のMultiHeadに与える入力は何が異なるのか？\\u3000\\\\nなどなどです。\\\\nよろしくお願いいたします。', '各回の概要の紹介がよかった。', '特になし', '話が聞きやすく、長めの講義時間だったがストレスなく受講することができた。不満は特にありません。', '他の回と少し話の種類が違うかもしれないが、半導体の話も楽しみにしています。', '説明がゆっくりで、かつ明瞭であり、プレゼン技術習得としても参考になった', '今回はとても平易かつわかりやすい説明を意図されているように感じたが、昨年の内容から推測するに、Day2以降から急激に難易度が上がることが少し心配です。', '視覚的に理解できた部分もあり、また説明が的確でとても分かりやすく拝聴しました。', 'ありがとうございました。一生懸命取り組む所存ですのでどうかよろしくお願いします。', '周辺知識（様々な分野のモデルの概略や、日本の状況、GPU環境など）にも触れていて、内容が身近に感じられたのが良かったです。', '特に、Scaling Lawによって見通しが立てられることについて時間をかけて触れてもらえて有益だった。', '開発コストに関する視点や日本の現状など、講師陣の丁寧な説明が分かりやすかった。RLHF(人間の価値観をフィードバックしてLLMに学習させる)を知らず、勉強になった。\\\\n', '特になし。強いて言えば、将来の松尾研が目指す姿を語ってほしかった。', '講師陣の説明は丁寧で、初学者の私でも理解しやすい言葉を選んでいたと感じる。', '受講の機会を頂き感謝申し上げます。初回を受講し、非常に充実した内容に驚きました。次回以降が楽しみです。体験学習によりLLMの学びを深め実践力を高めたいと思います。引き続き、よろしくお願いいたします。', 'これまでネットのニュース等で知っていた断片的な知識を自分の中でLLMという体系でつなげることができたと思います。', '講義内容の概要を知れた点', '特になし\\\\n不明点はチャットボットを利用して解消します。', '特になし', 'Overviewとして前半の講義内容を触れていただいて、期待度が上がっています。\\\\n予習、復習、発展的な学習を通してより理解を深めたいです。', '使用してみましたが、今の所問題ないです。', '今後の学習のイントロについて、初回に聞くことができたのは今後の受講しやすさに繋がる内容であると感じます。', '講義終了後すぐにZoomが閉じられてしまい、アクセスしようと思っていたチャット欄のリンク先を見失ってしまいました。\\\\n事務的な案内については別途Slackにて流していただけますと助かります。', '各講義の概要について、簡略的ではあるが基礎となる部分を教育して頂くことで今後何を学ぶべきか、また自学研鑽しなければならないことは何かを理解できた。', '現在のところありません。', '非常に聞き取り易い講義でした。', '現在のところありません。', 'まだ使用していないため、使用後記載させて頂きます。', 'これから毎週楽しみにしています。リアルタイムで見ましたが、アーカイブも見るつもりです。\\\\n運営・講師の皆さん本当にありがとうございます。', 'このアンケート項目が分かりにくかったので、改善可能であればぜひ改善をお願いします。\\\\n- リッカート尺度の5が良いのか1が良いのかは自明ではないので、書いていただけると嬉しいです。\\\\n- リッカート尺度が回答必須なのか任意なのか書いてあると嬉しいです。例えば、質問をしていないので質問への対応を評価できませんが、回答しなくてよいのか分かりませんでした（実際には回答必須だった）。', 'これだけ大規模な授業を行うための授業資料準備や予算・スタッフ確保は、想像もつかないほど大変だと思います。\\\\n学ぶ機会を与えていただきありがとうございます。\\\\n\\\\nちなみにとてもささいなことで恐縮ですが、B様の授業スライドで表記ミスらしきものに気づいてしまったので共有します。\\\\n自己紹介: 大学院の所属をスペースで区切りたい場合の位置「東京大学（スペース）大学院工学系研究科（スペース）HOGE専攻」（「大学院工学系研究科」で一つの名称です）\\\\nP24: aligne -> align?\\\\nP27: 高い効率の -> 高い効率を', 'スケール則の意義が、計算試験の予想がたてられ、予算が計上できるようになったことという新しい気づきが得られた。', '本日の講義の直前に配布されていた資料に追記されているページがあり、講義のスピードが速かったため、復習用に最新の資料の再配布が必要と感じました。ご検討よろしくお願いいたします。', '大変有意義な活用だと思います。是非継続していただきたいと思います。私も少しでも貢献できるよう頑張ります。よろしくお願いいたします。', '技術的な質問と、運用的な質問を一つのチャットbotで答えるのは難しそうなので、分けてみてはいかがでしょうか？', '日本の現状について最新の情報が聞けたのがよかったです。', '予習のための資料が予め配られていることに気付きませんでした。もうちょっと派手に宣伝してもいいかも。', '不満は特にありません。みなさんさすがに話慣れているし、音量も適切かつ人によらず一定であることがよかったと思います。', 'そもそもLSTMではダメダメだった自然言語処理が、Transformerで急に良くなった理由を数学的に理解したいです。とにかく、これからの講義が楽しみでわくわくしています。', 'QAのことですね。いちいちキーワード検索するのはダサいと思います。質問を自動的に判定し、過去にあればそれを表示すればよろし。', 'RAGを使った推論結果を導きだすプロセスが個人的には特に良かったです。', '今回は初回という事もあって後半の講義は2回目以降の講義内容の紹介などで終わったので分かりにくかった所などは特にありません。逆に今後の講義が楽しみでワクワクしております。', '講師の方々も非常に丁寧に話されいてとても良かったです。', '大規模言語モデルに関して肝心な内容を纏めている', '昔の、把握しておくべき技術（Attention、Transformer）がわかった。', '特になし', 'わかりやすかったです！', '私自身利用していないのですが、回答ができない場合にもRetrieveされた参照情報が出力されているようです。LLMで生成された回答が参照情報をもとに回答できているかをLLMで判定し、回答できていない場合には参照情報を出さないようにするなどの工夫ができるかなと思いました！', 'llmの本格的な講義は初めてのため、ついていけるか心配ですが、よろしくお願いします', '日本企業はGPU争奪に出遅れているというニュースを聞いたことがあるが、GPUの高性能化によって後発の利があるという視点を聞くことができてよかった。', '今後の各回講座が期待できる内容で、学習意欲が湧きました。ありがとうございました。', 'まず、講義ではプリミティブな事項をシンプルかつ正確な表現でご解説いただき、必要に応じて詳細な文献等に当れるように構成していただいている点が大変勉強になります。', '今回は初回ということもあり、内容的に分かりにくいところは私の勉強不足のところですので、すぐに補うように致します。', 'A先生にも、B先生にも、大変分かりやすくご解説頂けました。特に、B先生のご解説により、今回の講義の全体像をいち早く掴むことができたのは今後のモチベーションに繋がります。1年で非常に進展する世界ですので、昨年の講義内容を踏まえ今回の講義内容がよりブラッシュアップされ充実されたものになっていることはよく理解できました。楽しみです。', 'これから使用させていただきます。気づいた点はFBさせていただくようにします。', '計算環境、評価といった実際にLLMの実務で必要となることに関して、網羅的な説明', '実際に使われるためには、読み込ませているデータの明示とチャットボットのUI改善は必要かと思いました。データの取り扱いに問題ない状態にして、有志で取り組んでも良いかもしれないですね。', '授業の全体的なOVERVIEWを把握することができ、今後の学習意欲の向上につながりました。', 'ChatBotそのものがLLMの応用例として実用性に対する説得力を感じました。', '説明、解説が丁寧でとてもクオリティが高いと思いました。日本のLLM技術の底上げ、人材育成に多大な貢献になる非常に素晴らしい取り組みだと思います。', '全体を俯瞰的に説明していたこと。', '講義での質問の仕方は大事なので授業で話してほしかった。', '非常にわかりやすかったと思います。', '今、2回目で使うハギングフェイスを通じたモデルの取得で苦しんでいます。グーグルコラボの設定が悪いのか、meta-llama/Meta-Llama-3.1-70Bを取ってきても落ちてしまいます。次回以降、PC環境を整備できるかが大きなポイントになるので、可能であれば、講義の中で十分にコメントしてほしいです。', 'まだ見ていません。', 'RLHFやMoEなどは名前は知っていたが、どのような手法なのかイメージできていなかったので、非常に勉強になりました。', '最初の基本的な知識について、おさらいすることができて非常によかったです。', '最終回のロボット応用が楽しみです。期待しています。\\\\nなぜこれが無料なのか、どこからお金が出ているのかについても言及してほしいです。\\\\n\\\\nSlackが限定チャンネルになっていないのがわかりづらいです。この講義の会話をしているのであれば、リンクが貼れた方が便利です。', 'Scaling Lawについてなんとなく知っていたが、自分の課題としてとらえることができた。また、理解していないことがいっぱいあることも再認識できた。', '特にありません。', 'お話も資料も、大変わかりやすかった。このプログラムに参加できることに、あらためてワクワクしました。', '使ってみます', 'LLMをチューニングする技術については市場に情報が出回っているので、ある程度知っていたが、前処理部分についての情報や半導体技術についての情報は面白かった。', '特になし', '特になし', '特になし', '講義の全体像をわかりやすく解説していただいたので、事前に何を予習しておけば良いか理解できたのがよかったです。', '特にありません。', '非常に明確で、複雑な概念も分かりやすく解説していただきましたので理解が深まりました。', 'ドメイン特化のファインチューニングの方法を詳しく学びたいです。\\\\n上手なデータセットの作り方、データセットの失敗例なども学べると嬉しいです。', '質問に対して参考リンクを貼り付けていただいていると思うのですが、「今回担当した講師の名前は？」のようなBotが知り得ない情報に対する回答にも論文のリンクが貼られているのが気になりました。条件分岐などを利用し一般的な質問、専門的な質問、講義に関する質問を分けて回答できるようにすると良いかと思いました。特に講義に関する質問がこれから増えてくると思うので、RAGなどで講義内容を文字起こししたコンテキストなどを取り出して回答するように出来れば、ユーザー満足度が上がりそうだと思いました。', '今後の予定をわかりやすく説明してくれて、このさきの講義に臨む心構えができました。', '特にありません。', '分量もちょうどよく、全体的に不満はありませんでした。', 'このあとの講義もとても興味深い内容だったのでとても楽しみにしています。', 'チャットボットで質問をしましたが、スケール則にでてくるパラメータ数とは何かの説明があまりなくてわかりにくかったです。\\\\n計算資源、データセットサイズについては言葉だけイメージし易いですが、パラメータ数については何を指しているのかがイメージしにくかったです。', '今後の講義内容の概要を説明していただいたことによって、今後の見通しが分かり、学習の心構えができました。', 'Scaling LawとEmergent Abilities。業務で研究開発中の画像認識モデル（非LLM）にも同様の考え方が適用できるのであれば、synthetic dataを大量に生成して事前学習したモデルをreal dataでfine-tuningすることで限界突破できないか？と思ったため。', '私自身は事前知識があったのでスムーズに理解できたが、初学者の方も受講していると考えると、lossなどの基本的な用語集や予習リンク集等を事前に配布した方がよいかも？', '全体が網羅されていて非常にわかりやすかった。一部ニューラルネットについては、CNNやRNNなどさらっと流した部分があり事前理解がないと厳しい部分もあったが、総じて浅く広く全体感を学ぶのに最高のコンテンツであった。', '講師は、特に世界の言語モデルがいかに多様であるかについて多くの情報を提供しました。', '特にないです', '講義全体を俯瞰的にご説明いただいたので、キーワード間のつながりが理解できたのが特によかったです。', '次回からの詳細な内容を楽しみにしております。', '全体感が把握できたこと。より興味が高まった。', '音声について、十分クリアであったが、ヘッドセットを用いて頂けたらさらにクリアになるかもしれない。', '今後授業の概要を教えてもらって良かったです。何か勉強するかのイメージができました。', '各回でどのようなことをするのかがわかりやすかった．知らなかったテクニカルタームもいっぱい知ることができた．\\\\nまた，日本語のLLMの取り巻く状況については，詳しく知ることができた．', '特にありません．', '特にありません．', 'これからの学習がとても楽しみです．', 'Tanukiについて説明していただいたのがとても印象的でした。フルスクラッチでGPT-3.5TurboクラスのLLMが作れると言うのは私にとっても明るい未来を感じる内容でした。', '全体的にとても分かりやすく、知らないワードは説明していただいてたと思うのですが、「パラメータ」と言うキーワードだけ余りに常識的なのか説明がなかったように思います。何となくモデルの大きさなのかなとは思いましたが十分な理解をしていないので自分で調べておこうと思います。', '皆さまとても真剣に「伝えよう」と言う気持ちいっぱいでお話ししていただきそのパワーをひしひしと感じました。欲を言いますと、最終的には時間内に終わったので何の問題もないのですが、恐らく時間配分が予定と違っていたのだろうと言うところまで伝わって来ました。ペース配分に余裕を持って進められると安心して講義内容に集中出来ると思います。', '下手の横好きで囲碁のルールを知っていますが弱いです。以前から「盤面を使って言葉で教えてくれるAI」を作ることを夢にしております。今回の講義を聞いて、まんざら非現実的な夢でもないと感じました。夢を実現するためにその基盤であるこの講座をしっかりと身につけようと思っております。', '講座の後に「パラメータって何ですか？」と言うのを聞いてみようと思ってたのですが、先にこのアンケートに着手してしまったためまだ使用出来ておりません。後で触ってみようと思います。', '今回のLLM講座の全体像を丁寧に説明いただけたので、LLM初学者の自分でもよく理解できた点が良かったと考えます。', '例えば、後半スライドの13ページ目で出たLossが具体的に何を指しているのかなど、参考に数式などを示していただければ尚良かったと存じます。（おそらく、クロスエントロピーのことを指しているものと解釈しました。）', '次回のDay2からは演習や宿題でコードを実装する機会が多くなると考えていますので、事前学習の部分をコーディングを通してよく理解したいと考えています。', '各回の概要を詳細にお話いただきましたが、心構えとモチベーションを高めることができました。', '近年，大規模言語モデルに投資が集中している背景がよく分かった点．', '文系出身の人でもわかる表現などがあるとありがたく存じます。', '金融などドメインに特化したLLMおよびロボットに対する実応用など、AIの社会実装に向けた取り組みを多数ご紹介されていた点。', '特になし', '特になし', 'まだ利用していないため、利用後FBいたします。', '生成AIサービスの利用に関する注意喚起は、総務省ではなく、個人情報保護委員会が発出しているものだと思います。', '具体的にLLM開発においてどのような資源が必要かの概要が', 'きわめてわかりやすかったです。', '講義スライドは提供されますかの答えがあいまいでした。すみません、あとで受講の手引きをよんでGDRIVEにあることはわかりました。答えとして”受講の手引きを再確認してください、共有できる資料はそこに案内があります”みたいなこたえがよりよかったです。', '講義で特によかった部分は、実際の応用例を通じて大規模言語モデル（LLM）の力を実感できた点です。例えば、医療分野での診断支援や、カスタマーサービスでの自動応答システムなど、具体的な事例を交えて説明されました。また、最新の研究成果や技術トレンドについても詳しく解説され、今後の可能性についてのディスカッションが非常に興味深かったです。', '特にありません。', '講師についてのよかった点としては、専門知識の豊富さと分かりやすい説明が挙げられます。特に、複雑な概念をシンプルに解説し、具体例を交えて説明することで、受講者が理解しやすいように工夫されていました。また、質問に対しても丁寧に答えてくれた点が好評でした。', '特にありません。', '以下の点が改善されると、さらに良い体験が提供できるかもしれません：  応答速度の向上：リアルタイムでのやり取りがスムーズになると、ユーザーの満足度が高まるでしょう。 自然な会話の流れ：より人間らしい会話の流れを意識すると、ユーザーが親しみやすく感じるかもしれません。 多言語対応の強化：日本語以外の言語にも対応できると、より多くのユーザーに利用してもらえるでしょう。 ユーザーインターフェースの改善：使いやすいインターフェースやデザインも重要です。 パーソナライズ機能：ユーザーの興味や過去のやり取りに基づいて、より個別化された応答ができると良いですね。 これらの点が改善されると、より多くのユーザーにとって有益なツールになると思います。', '本講座の各回の概要を聞くことができ、また、LLMを取り巻く環境についての説明の中で、日本のLLM開発が海外に遅れを取らないように進化、加速していることが分かり、これから本講座を受講するのがとても楽しみになりました。', '事前にダウンロードした資料と、講師が使用していた資料に差異が見られたので、なるべく同じものを提供いただけますと幸いです。', '全体を通して、難しい言葉を使わずに分かりやすく教材の内容を説明されていたのがとてもよかったです。', 'これから４か月間、よろしくお願いいたします。', 'これから講義が進むにつれて古い質問と回答が埋もれていくと思うので、講義回ごとに質問をフィルタできるといいなと思いました。', '今年に入って生まれた最新のサービスの説明や、昨年の講座からのアップデート箇所の紹介があり、今年この講座を受けられるメリットを強く感じられたこと。', '「講座範囲内/外」の意味が若干わかりにくいように感じましたので解説をいただけると助かります。例えば、講座で説明されている内容をより嚙み砕いて知りたい場合は「範囲内」、講座で言及されたトピックについて発展的な内容を知りたい場合は「範囲外」という理解でよろしいでしょうか（それとも後者も「範囲内」でしょうか）。また、範囲内/外により何が変わるのか（参照するDBが変わるのでしょうか）、間違うとどうなるのか（精度が落ちる？）も気になりました。', '海外と日本のLLMを取り巻く状況の違いについて言及するようなことはあまり知る機会がなかったので、とても勉強になりました。\\\\nまたGENIACや研究室でやっている取り組みやその状況を当事者として発表されているので、新鮮で正確な情報を提供していただいた点も良かったです。', '丁寧で良かったです。', 'LLM講座の全体像が自分なりに把握できました！個人的には、自民党の勉強会資料でも記載のあった今まで分からなかったスケーリングLawのグラフの見方が分かりました！', 'A先生のスライドp10のトランスフォーマーの図で、並列で扱いたい部、長いものを扱いたい部の図の意味が分からなかったので、自分なりに調べてみたいと思いました。トランスフォーマーの仕組みが不思議だったり、自分の分からないところが分かってきたので、とても楽しいです。', 'A先生、B先生が、難しいことを、噛み砕いて教えてくださったので、受講生としては、分かり易かったです。裏を返すと、講師の先生が凄い大変な思いをして、説明やスライドを作って下さっている講義や資料の準備の苦労が伝わってきました。', 'チャットボットをあまり使ったことがなくて、どういう風にチャットボットに質問したらよいか、聞き方を知りたいです。それが分かればチャットボットに上手に質問できるようになるのではないかと思います！', 'これからの講義全体的な事前知識の勉強ができたのでよかったです', 'まだ使ってないですが、気軽に質問できるのでとてもいいとおもいます！', '大規模なモデルの学習をする際に、モデル並列化のコンセプトで学習できるのが面白かった。趣味レベルのAI開発ではGPU一枚で完結しているため、知らない領域の話で興味深い。大規模なモデルの開発は、それ専用の開発手法があり、より深堀りして学びたいと思った。', '全体的なトピックを包括的に学べました。', '画質も良いですし、非常に聞き取りやすかったです。\\\\n内容も今回の講座のために新しく作成していただいていることがわかり、意欲が増しました！', 'ユーザ名や日付などを打ち込んで検索する際にEnterを受理していただくとさらにUIが良くなると思いました。', '全般的に聞きやすく、全体の流れをつかむうえで必要十分だったと思います。\\\\n改めてAIの応用課題ごとのテーマについて考える機会になりました。', 'とても良い。', '・話が変わったりする講義の構成だったので、最初にどんな流れで話を進めていくのかのアジェンダがあると良いと思いました。', '・話すスピードが心地よく、頭に入ってきやすかったです。', 'LLMに関する基礎となる技術に概要を知ることができました。', '基本的な用語も補足しながら説明いただけたので、とてもわかりやすかったです。', 'B先生によるこれから学習する内容のラップアップが全体像の掴みやすいもので大変参考になった', 'Fine-TuningやRLHFについては試したことがないので、非常に興味が沸いた。', '各回の説明があり、これからの講義内容についてイメージができた点', '最初のほうの生成確率についての説明がシンプルかつ根源的な内容なのだなーと感じて面白かったです。', 'LoRaというのは、SVDしたものを行列圧縮したものなのでしょうか？(rの値がどう決められるのかが気になりました)', '前半では、LLMの確率モデルが明示されたこと、LLMの基本原理ともいえる巨大化や汎用性が提示されて式が整理されたこと。後半では、RAGの仕組みや、Pre-trainingの要点、RLHFの順位付けの規準が、それぞれ簡潔に理解出来たこと。', '特になし。', '初日、各回講義の概要をさらっていただき有難く思っております。自身の学習不足を実感し、改めてセミナーへの心構えを持つことができました。', 'スライドとご説明において、特段不明瞭であったり見にくいといったことはありませんでした。', '聞き取りやすく、また丁度よいテンポで進めてくださっていたと思います。ありがとうございます。', 'LLCにおける米国との比較について、非常に参考になった。', '特になし', '資料について図やイラスト、参照資料などが豊富であり、非常に理解しやすかった', 'これからよろしくお願いします', '情報量が多かった', '事前学習、事後学習、RLHF等のLLMの学習段階のフェーズがわかりやすく説明されていたこと。', '各回の概要を説明してくれたので全体像が掴めることが出来た。特に楽しみなのは第三回のPre-traningで紹介のあったAttention機構の動作を説明する動画である（理由：Attention機構のイメージが難しかった為）。', 'LLMのScaling則の３つの変数である計算資源C, データセットサイズD, パラメータ数Nの具体例を挙げて頂くともっと良いと思う。因みに計算資源Cとはメモリ容量と演算能力(GPUの数)と考えています。', '説明が上手いし、PowerPointの資料が良いので今後の講義が楽しみです。', '優秀な講師の方の講義を受講できることに感謝し、しっかり勉強して本講座を修了できるように頑張ります。9/10(火)から始まる日本語LLM、Tanukiの開発報告も楽しみにしています。', '未だチャットbotを使用してませんので、今後、使用したいと思います。', 'スライド22 のPre-train, Prompt, Predict の箇所で、モデルの転換と性能向上について分かりやすく説明くださったこと。', 'A先生もB先生も、とても楽しそうな雰囲気で語っておられ、適切なご説明のみならず、エモーショナルなお話しぶりもあいまって、聴講する側にとって、とてもすんなりと親しめることが出来ました。貴重な機会に恵まれ、誠に有難うございます。', 'チャットボットがあったとは存じ上げませんでした。いつの間にかたくさん質問が上がっているので、目を通してみたいと思います。自分でも無料版の Google Dialogflow でチャットボットを作ってみてはいるのですが、いかんせん世代が古いです。', '特によかった部分は、Transformerの誕生によるブレイクスルーと、その後のGPTモデルへの応用についての詳細な説明です。また、Scaling則やEmergent Abilityといった、大規模モデルの性能向上の原理についても非常に興味深かったです', '特に分かりにくかった部分はありませんが、大規模モデルの具体的な実装や最適化について、もう少し詳細な例があるとさらに理解が深まると思いましたが、そこら辺については今後の講義に期待を持てるガイダンスだったかなと思います。', '講師の皆さんは非常に熱心で分かりやすい説明をしてくださり、大変よかったです。複雑な内容も具体例を交えて説明してくれたので理解しやすかったです。不満点は特にありませんでした。', 'この講義は非常に充実しており、最新の技術動向を学ぶ良い機会となりました。次回以降も期待しています。また、実践的なプロジェクトやハンズオンセッションも含めていただけると、さらに実践的なスキルを習得できると思います。', '最先端のLLMの学術的もしくは実用的な例について、反復しながら学べたこと', '特になし', '概要が整理できた', '初見の単語が多く苦労した', '時間が短すぎると思う\\\\n', '前回の修了者６００名の属性を知りたい', '回答は適切だと思う', '・直近の情報まで扱っており、情報の鮮度が高かったこと\\\\n・具体的な論文名を挙げており、興味のある事柄について1次情報にアクセスが可能そうであったこと', 'プロンプトとスケーリングの関係性、そしてSupervised Fine-Tuningにおける専門的領域とサービス的領域の差異について学べたことが特に良かったです。これらの概念が互いに関連し合い、LLMの性能と応用に大きな影響を与えることを理解できました。専門的な領域とサービス的な領域でのLLMの活用の違いを知ることで、各分野に適したアプローチの重要性を認識しました。', 'スケーリング則について：\\\\n計算式が出てくるような部分が非常に難解で、理解するのに苦労しました。(ほとんどできませんが) 特に、統計的な予測を用いてLLMの性能を推定する方法は、理論的には興味深いものの、実際に応用するとなると非常に難しそうだと感じました。この知識を使って何かを実践しようとすると、相当な困難が予想されます。', 'いつも通常企業プレゼンを聞き慣れていて、社内でのプレゼントがと違って、中身の内容が非常に抗議的で良かったと思っています。GoogleやAmazonのセミナーと違って非常に最先端の研究者から聞けてよかったです。', '今回の講義に参加させていただき、誠にありがとうございました。\\\\n\\\\n私自身はプログラミングにはあまり馴染みがない状態で参加しましたが、講義を通じてAIやLLMの重要性と可能性を強く感じることができました。特に印象に残ったのは、プロンプティング技術やスケーリング則、そしてSupervised Fine-Tuningなどの概念でした。これらの知識は、AIの基本的な仕組みを理解する上で非常に貴重なものでした。\\\\n\\\\n一方で、ビジネスモデルの構築や新規事業のDXに関しては、25年間の経験があります。ビジネスを組み立てることや、マネタイズの戦略を立てることには自信があり、この分野ではある程度の知見を持っていると自負しています。\\\\n\\\\n今回の講義で学んだAI技術と、私のビジネス経験を組み合わせることで、AIを活用した新たなビジネスモデルや、既存ビジネスのAIによる変革など、興味深い可能性が見えてきました。特に、LLMやDifyのようなツールを使って、どのように新しい価値を創造し、それをマネタイズしていくかについて、多くのアイデアが浮かんでいます。\\\\n\\\\n今後、AIがますます私たちの生活や仕事に浸透していくことを考えると、データの重要性とプログラミングスキルの必要性を強く感じています。そのため、これからは娘たちと一緒に、少しずつではありますが、データ取扱やプログラミングの勉強に取り組んでいきたいと思います。同時に、私のビジネス経験をAI分野に活かし、新たな価値創造の可能性を探っていきたいと考えています。\\\\n\\\\n最後になりますが、最先端の技術と知識を分かりやすく教えていただいた研究室の皆様に心から感謝申し上げます。この経験を活かし、AIの発展と共に成長し、ビジネスとテクノロジーの融合による新たな価値創造に貢献していきたいと思います。\\\\n\\\\n今後ともよろしくお願いいたします。', '興味があるファインチューニングやRAGについて概要を知ることができ良かった。今後のモチベーションにつながる。資料の中でTanuki-8×8Bの会話の例があったがかなり日本人に親しみやすい返答で驚いた。デモンストレーションを試してみます。', '次回から演習開始なので楽しみです！', '理論とハードウェアの進化が両輪になっているところとても興味深い。\\\\nScaling-Lawが主流とは思われるがSmall Language Modelの発展・展望も追いかけたいと思った。', '全12回の講義のロードマップがわかり、ついていけるか不安でしたがステップを踏んでいけばゴールできそうと一安心しました。\\\\n', '全体が平易にわかること。', 'わからないことは今後の講義でわかる認識です。', '各章の概要だけかと思っていたら、サマリをわかりやすく説明してくださり、とても勉強になりました。', '体系的な説明をベースとしつつ、最新の状況がキャッチアップされていた点がとてもよかったと思います。', '特にありません。', '講義資料で把握できる内容を、わかりやすく補足しつつ説明いただいたので、とても理解が進みました。', 'RFHFやMoEといった用語については、言葉として目にしたことはあるものの、きちんと調べたことがなかったため大変参考になりました。', '資料は読みやすさや理解のしやすさに配慮されていて過不足ない内容と感じましたし、口頭での補足情報を含めて丁寧に解説いただいたので比較的分かりやすかったと思います。', '以前、貴学の講座によりDLを学んだときはCNN（画像）に重点を置いて学びましたが、あっという間に時代が進み、世の中の興味は生成AI、特にLLMの利活用にシフトしてしまいました。自身の先見性のなさに嫌気を感じつつ、一から学び直すつもりで受講させていただいております。どうぞ、よろしくお願いいたします。', 'LLMを「活用する（できる）技術」として捉えるために、技術的な背景や原理・限界を学ぶ必要があることがわかりました。\\\\n今後の一連のカリキュラムでそれらを学べることが分かり、受講のモチベーション向上につながりました。', 'GPT-4のリーク情報等は貴重な情報でした。', '特にありません。', '説明が大変分かりやすかったです。', 'データセンタの電力や地球温暖化等、LLMの発展には不安な面もあります。', 'まだ使う機会は無いですが、気軽に使ってみたいと思います。', '自身で投稿することはまだないですが、他の方の質問投稿を拝見しました。回答を見て私も勉強しましたが、興味深かったです。何か質問してみようと思います。', '「創発的能力」など知らない情報を得られた。', 'なし', 'なし', 'ある程度の事前知識は必要だったかもしれませんが、これまでのLLMに関する重要なトピックと直近の動向を研究者である講師の方々から直接お聞きできた点がよかったです。', '特にありませんが、本日は概要の把握のみであったため、詳細は各講義にて学ばせていただきます。', '話すスピード、話し方が聞きやすかったです。', '本日はありがとうございました。次回以降もよろしくお願いいたします。', '今回、特に知りたかったのは、RLHFやAdvanced Pre-trainingで、エンジニアリング的にはdeepSpeedの内部構造や詳細な使い方の理解である。\\\\n求めている技術内容もしっかりと取り上げてくれる予定で、非常にエキサイティングに感じている。', '全体的に分かりやすく、内容もOverViewの段階にも関わらず非常に濃かった。\\\\n時間も厳守して終了されており、特に改善点は見当たらない。', '言葉も明快でわかりやすく、全てにおいて良かったです。有難うございます。', '今回、このような濃い内容を無料でオンライン展開して頂き、ただ感謝しかありません。\\\\nぜひ何かをつかみ取り、今後の開発に生かしたいと強く思います。\\\\nどうか宜しくお願い致します。', '来週からも楽しみにしています！', '概念的な部分に関して少しお話しいただいたので、より各回の内容が楽しみになった。', '難しい言葉がなくわかりやすかった。', '各回の概要を示していただいたことで、各回で何を理解すればよいか整理できた。', '今後の流れが十分理解できた。', '現在準備中となっていた後半の講義範囲についても説明してもらえたら、さらに良かったと思いました。', '声のトーン、スピードがちょうどよく聞き取りやすかった。\\\\nスライドを読み上げるだけの形でなく、とてもためになった。', '去年の講座の修了率が30-40％というのは驚いた。\\\\n修了できるよう、努力したい。', '各回の概要について、具体的な内容のイメージにも触れていただいて、今後の受講に対してとても期待が高まりました。', '大規模言語モデル等の基本や講義の全体像について、複数の講師の先生方によって、体系的に伝えて頂いた点', '特にありませんが、細かい点で、「忌避技」は、「忌避肢」かと思われました。\\\\nTransformerの構造など、今後の講義で詳しく扱われるということでしたので、予習していきたいと思います', 'よかった点：要点だけでなくどこにLLMの強みがあるかなど、応用面も含め、各先生方の講義が有機的につながっていた点', '各回を事前におさらいしていただくことで、何を学べるのか、何を理解できていないのか、それによって予習しておくべきことがわかり、意欲も増した。', '特にありません。', '特にありません。', 'レビュー的に概説をお話し頂き、発展が速すぎてキャッチアップできていないLLM領域についてクイックにご紹介いただけて良かったです。線形なLLMの精度向上の話など、重要な箇所を繰り返してくださったので良く理解できました。\\\\n', 'ゆっくり話してくださったのでありがたかったです。', '各回ごとに今後どのような構成で講義が行われるのかをLLMを扱うための流れに沿って説明していただいたことがとても良かった。', 'お二人共説明が非常にわかりやすく、速さもちょうどよかった。', 'LLMの仕組みは実際的にはかなり複雑であるはずだが、初学者にもイメージしやすいように説明してくれたこと。', 'インターネット上のテキストが学習に使われるときに、重複や誤情報もそのまま飲み込まれているのかが気になった。\\\\n中国はAIの発展が著しいとよく聞くが、LLM分野ではどうなのでしょうか？', 'みなさん、聞きやすい声だった', 'このアンケート欄の中に、「もっと知りたい部分」を聞く設問があってもいいと思う。具体的には、「分かりにくかった部分や改善点、、、」の設問に付け加えるのはどうでしょうか？', 'LLMとは一体どのようなものなのかについて概要を初めに説明し、その後各講義で取り扱うLLMの細かい部分を大まかに一通り説明していたところ。', 'LLMの発展してきた流れがそこで使われている技術と共に学ぶことができた', '特にありませんでした。', '特にありませんでした。', '第一回ということもあり、今後の流れや詳細をあらかじめ端的に説明してくださったため事前学習ができる。', 'スライドにイメージしかなく説明は全部口頭だけの時があったため、少しキーワードなどもスライドにあったら助かります。\\\\n', 'スピードが初心者でもついていける速さでちょうどよかった。', '本日はありがとうございました。今後ともよろしくお願いいたします。', '内容がよく整理されており、理解しやすかった', '内容が多く、かつ進行が速かったので少し理解しづらいところがあった。', '言葉遣いがしっかりとしていてとてもよかった', '初回の授業を、楽しんで受けることができました。引き続き、よろしくお願いします。', 'LLMに必要な学習データと計算量の規模感を数字でわかったこと。', '3つの異なる学習があると言う事。事前学習、事後学習、RLHF。', '受講の手引きと言うものがどこにあるのかがSlackからは分からない。\\\\nメールをひっくり返して探してようやく見つけられた。\\\\n受講の手引きへのリンクをSlackに載せることは出来ないか？', '不満な点はない。有用な講義を頂きありがとうございました。', '日本語でこのような学びのチャンスが無料で与えられていることに感謝します。社会への貢献を頂きありがとうございます。ぜひとも生かしたいと思っております。', 'チャットボットがどこにあるかわからないのでまだ利用していません。もっと分かりやすい場所に手アナウンス願います。', 'スライドも見やすく、事前に資料として提供いただけるところが予習や内容把握のためにも非常にありがたい。スライドも文字ばかりではなく、図やグラフなども差し込まれており直感的に分かりやすい説明で理解しやすかった。', '特にありません。', '限られている時間の中であったにも関わらず、丁寧に聞き取りやすい説明を意識されていると感じた。内容や進め方に対しても特に不満はありません。', '今後の講義内容や課題を通した自身のスキルアップが楽しみです、よろしくお願いいたします。', '特に気にならず、どこで使われているかイマイチ分からなかった。', '大規模言語モデルの全体像，そして本講座の全体像をお教えいただき，LLMに対して勝手に抱いていたイメージを払しょくし，これから学んでいけることがわかり，とてもよかったです．', '特にありません．', 'ご自身のプロフィールからお話しいただくことができ，LLMを身近に感じることができました．', '言語モデルの仕組みだけでなく，学習に用いるデータやGPUについても触れられていた点．', '講義では省略された計算量に関する資料が，アップロードされている資料に含まれていなかった．', '日本のLLMの現状について知ることができたこと。', '資料がわかりやすく見やすかった。', '特にありません。', '分かりやすい解説でした。', '適正な回答であるか本人では解釈できない場合があるため、新たなカテゴリーなどを作るか、フラグなどを作っていただき、運営回答などが欲しい。', '話が聞きやすかったため、このまま続けて欲しい。', '説明に使用する図が非常にわかりやすかった', 'LLMの精度はデータ量、計算量に比例するというのがよくわかった。', '2時間ぶっ続けの講義なので途中で小休止がほしい。', '特にない。', '休憩を入れてほしい。', '特にない。', '日本でのLLM開発の進捗状況についてほとんど知らなかったので勉強になりました。また、全体像を詳しく知れたことで、モチベーションが上がりました。', '質問の文字数を増やして欲しい。また、改行をできるようにしてほしいです。', 'また、第２回から第８回までの学ぶ内容をタイトルだけでなく、簡単な内容まで含めて紹介することは全体像を捉える上ですごく役に立った', 'どの先生も説明がわかりやすく良かった。講師の先生によって声の大きさが小さく聞こえたりするので最初にマイクの調整をしていただけたらなと思いました。zoomでこちら側からchatできないので。', '12回と短い期間ですがよろしくお願いします。', '日本におけるLLMの開発状況について、具体的なモデルを題材にして、使用した計算資源についてを含め非常に具体的な内容で参考になった。', 'とくには大丈夫です。', '日本のLLMを取りまく環境について、スケール化が求められていることがわかった。', '主要な論文が取り扱われていた点', '今日は興味付けや概要の伝達が目的の回だったと思いますが、それらが分かりやすくなされていて助かりました。', '日本語の評価指標はあまり詳しく知らなかったのでとても興味深かったです．', '今日の講義で特に印象に残ったのは、「Transformer」の仕組みとその応用に関する説明でした。2017年に登場したこの技術が、言語モデルの性能を飛躍的に向上させ、特に翻訳や質問応答などの分野で目覚ましい成果を上げているというのは、とても興味深く感じました。\\\\nまた、自己回帰型モデルとの違いについても分かりやすく説明していただいたおかげで、現代の大規模言語モデルの多くがなぜこの技術を採用しているのか、よく理解できました。\\\\n中でも「Attention is All You Need」という論文の紹介は、理論的な裏付けと実用性のバランスがよく取れていて、非常に勉強になりました。', '今日の講義で少し分かりづらかったのは、自己回帰型言語モデルとニューラル言語モデルの違いに関する説明でした。両者の概念自体は理解できたのですが、実際にどのように実装されるのか、またどういった場面で使い分けるのかという具体例がもう少しあると、より深く理解できたかなと思います。', 'これから活用していきたいと思います。', '研修関するアナウンスがバラバラなって、メールに記載してるものとSlackに記載してる内容も異なります。\\\\n可能なら、ルール関するアナウンス内容を同一して欲しいです\\\\n練習notebookを実行するために、課金しないとダメです。社会人として研修を参加してるので、まとめて料金を請求するような精算しやすい方法の検討をいただけたら助かります', '来週以降行う内容が包括的かつ具体的に示されていた点。', '質問に対し、正確に答えてくれていた。', 'ところどころ英語が出てきて、わからないところがありました。私が勉強っできる範囲でがんばっていきたいです。', '分かりやすいように説明などがあってよかったです。', '最初はなかなか追いつかなったが、質問、説明などのおかげで、自己帰言語モデルは、自身の生成物を用いて自己改善するAI技術だということがわかりました。', 'なし', '資料が見やすく、分かりやすい。', '特になし。', '説明が簡潔であり、とても分かりやすかった。', '第二回と第三回と第五回がとても楽しみです。', '本講義で更なるLLMの勉強意欲が上がりました。今後ともよろしくお願いします。', '・LLMがなぜここまで普及しているかの理由（スケール則，プロンプト等による汎用性能）や，LLMのここ数年の状況の概要を知ることができた．\\\\n・後半に今後の講義の全体像を解説している点が，全体の状況を把握することできたのでよかった．', '注意点の一つで，本講義に対するURLはslackで通知しないようにとのことだった．あまりその認識がなかったので講義前にうっかり通知する人がいないか心配になった．よく見ると手引きにも書いてあったが，見逃す人がいるかもしれないので，もう少し目立たせても良いと思った．', '今後の講義で触れていく内容の概要について解説していただいたので、今後の見通しが詳細になったことです。', 'LLM研究・開発が普及している理由について、全体像の概要を把握できたこと', '紹介してくださった技術はどれも複雑なものであるにもかかわらず、とても簡潔で明瞭な説明がされており、非常に理解がしやすかった。', '先生方のお話しは、簡潔かつ明瞭で、とても分かりやすかった。\\\\n一般論だけでなく、先生個人としての意見や考え、興味も少し聞いてみたいと感じた。', '今後の流れについて分かり易くお話いただけて良かったです。', '聞き取りやすいスピードで話していただけて良かったです。', '第6回講義、とても楽しみです。', 'たぬきがとても好きなので、日本発のモデルの名前がたぬきでよかった。', '各回の概要をわかりやすい言葉で聞くことで、自分が学びたい部分を明確にすることができた。', '各回の概要をある程度どんなことかを説明してくださってたこと', 'AIの仕組みについて学んだ際に学習データの重要性は習ったが、どの程度のデータ量でどれくらい精度が上がるかまでは理解できていなかったため、大規模言語モデルは膨大なデータ量を事前に学習することで回答精度を上げていることが理解できて良かった。学生時代に情報処理を専攻していた頃は、C言語等の潤沢なリソースがなくリソース管理に気をつける前提の言語から、Java等のリソースがある程度潤沢にあり管理を気にしないタイプの言語にトレンドが移りつつあるような頃だったため、ここまで潤沢にリソースを使うことで大規模かつ高速処理を実現するような時代に変わったことを知ることができ、ハードウェア技術の急速な発展を実感した。第6回の半導体エコシステムの講義回も楽しみに感じている。', '本日の講義では特にわかりにくい点はありませんでした。', 'A先生：言語モデルの定義や大規模言語モデルまで発展した流れ、急速に発展した要因など大規模言語モデルを学ぶ上でのベースとなる知識をわかりやすく説明いただいた。\\\\nB先生：各回のポイントをわかりやすく説明いただき、各回で学ぶ内容の大枠を理解できたと感じた。各回を楽しみに思えるような説明で良かった。', 'LLM初心者でもLLMについておおまかに知ることができた。', '10回にapplication of llmsの回があると知り、興味を持った', '全体がみえてよかった\\u3000わかりやすかった', '概観という形で今後の講義の内容を先取りできたのは有意義だった。現段階で大まかなLLMの知識を吸収できたため、以前よりもLLMに対する理解が深まったと思う。', '講義で使用しているスライドの一部が、配布スライドに含まれていなかった。可能であればすべて収録していただけると助かります。', 'LLMについての前提知識が浅い私でも、わかりやすい説明だった。', '強いて言うのならば、昨年度のデータのまま前置きをして説明するのではなく差し替えてほしいと思うタイミングはあった。\\\\n一方で昨年度のデータも参照できたことで１年間での進歩が伝わりやすかったため、必要に応じて差し替えてほしい。', 'LLMとりまく環境を含めた概況がおおよそ理解できた。Web等での情報に比べ体系化されている為、理解しやすかった。', 'このような貴重な講義を受講できる機会をいただき、ありがとうございます。受講内容を十分に吸収し、今後の研究に活かせるよう努めたいと思います。', '初回の講義で、講義全体の流れを示していただけたので、到達目標が分かり学習意欲につながった。', '知識がなくてもこの講義一つで概要をつかめる説明になっていた。', '資料がすごくわかりやすかった。', 'scaling lawのところを初めて知りました．お金と性能から投資するという理由が興味深かった．スケールすると能力が上がる！！→投資\\\\nRAG：参照しながら文章を作る．ハルシネーションが起きにくくなる', 'トークンの説明の部分の例がとても分かりやすかったです．', 'わかりやすかったです．沢山吸収して自分のものにしたいです．', '講義全体の説明があることで、この先どの様に学ぶのかイメージがしやすかった。', '最後にまとめを出してくれたのが分かりやすかった。また参加者のビデオとマイクをオフにしてくれていたので、万が一を気にせず安心して受講できた。', '特になし', '聞き取りやすい音量、音質だったので快適に受講できました。', '初学者にもわかりやすい、丁寧な説明を聞くことができて良かったです。', '大学院で学んでいた、ベイズ的な条件付き確率で予測する話から、よく話題になるRAG, GPUの話のつながりがわかった点。', 'なし。', 'なし。大変わかりやすかった。', 'ありがとうございました！', 'まだつかってないので、疑問点が出たらつかってみます。', '自然言語回帰モデル、ニューラル言語モデルの課題付近の部分を真面目に視聴することができて、理解が深まりました。基礎の部分になると思いますが、ゲーム開発などに応用する際に演算を誤魔化すための手法(それっぽければ良い)を考えるのに役立ちました。', '講師の方皆様、話す速度も含めて聞きやすく説明もわかりやすかったです。', '初回ということで、LLMとはそもそもどういったものかの説明があったことがよかった。また、これからの展望などもあって学ぶ意義なども再確認できた。', 'わかりやすい説明でした。', 'LLM講座の導入として、最初になぜLLMを学ぶのか？について体系立てて要点を把握できた点が個人的に非常によかったです。専門が異なる人に自分の研究分野を説明する際に、なぜという観点で整理すると理解し易いだろうことを実体験として学びました。', '近年多く公開されている様々なLLMのサービスが授業中実例として度々紹介されていたため、技術と実用例のイメージが結びつきやすかったです。', 'LLMの変遷を知ることができ、大枠を理解することができた。\\\\n英語モデルを日本語翻訳するLLMでは、ELYZAベースが多いという点、知ることができた。', 'LLMが乱立しているため、特色や使い方、テクニックを学んでいきたい。', '特にありません。', '学習のモチベーションや目的を明確にすることで、より組織的かつ持続的な学びの姿勢を培うことができました。特に、LLM講座全体の構造や各講座の相互関連性を把握することで、学んだ知識の実践的応用がより一層容易になると感じました。この視座を持つことが、今後の知識の体系化や長期的な学習の促進に大いに貢献すると考えます。', 'これからの講座の概要について、話していただくことでなんとなく全体像が見えてよかったです。', '不満はありません。特にB先生の声が聞き取りやすくてよかったです。', '講義内容特有の内容だと適切ではない回答になっているように見えます。', '今後どのようなテーマについて勉強していくのか、アウトラインが最初に示されたことで見通しが立ったのがとても良かったです。初回から難しい内容で殴られずモチベーションも湧いてきました。', '特に無いです。', '特にございません。みなさん話がうまく、スピード、音量ともに聴きやすかったです。', 'これからどうぞよろしくお願いします！', '日本のAI開発の現状と世界との差を少し理解できた', '大変分かり易かったです', '説明が分かりやすく、導入として非常に良かったです。', 'これからよろしくお願いいたします。', 'LLMの進化の経緯を丁寧にご説明いただき、よく理解できました。', '特にありません。テキストも丁寧でよく理解できました。', '今後の講義内容に期待しております。', 'チャットBotまだ未使用', 'GPT−3で学習に使われたトークン数がどれぐらいのものか、わかりやすく説明されていて面白かった。', 'Transformerの仕組みを簡潔に説明し、従来のRNNの課題と解決を明確に理解できたことが良かったです。また、スケール化における課題解決が技術的な発展に直結している点も非常に興味深かった。', '事前学習とファインチィーニング、RAGの違いについて、もう少し具体的な実例を交えて説明があると、さらに理解が深まると思いました。', '講師の説明は分かりやすく、適度な専門用語の使用と実際の応用例がバランス良く取り入れられていて良かったです。', '今後の講義大変楽しみにしています。どうぞよろしくお願いします。', '有用なツールをご提供頂きありがとうございます。まだ十分に利用できていないですが、学習の補助として活用していきたいと思います。他の方が使われている状況を拝見すると、質問に対する回答の精度や深さなどが少し足りないかもしれません。自身でも試してみたいと思います。', '資料がわかりやすい', '本講座の今後の内容について浅く広く確認することができたので、受講のイメージがつき良い時間であったと思います。', '補足の情報を含めて聞きやすく、わかりやすい内容だったと思います。', '今後の講座の内容に期待していきたいと思います。', '講義全体はもちろん、初回overviewとして各回の概要がまとまっていたのが非常に助かりました。自分には少しレベルの高いものを受講しているという心持ちだったので、各回で何がポイントとなっているかがわかり、予習にも役立ちそうだなと思いました。', '初回ありがとうございました。良かった部分と重複するのですが、自分には少しレベルが高いかなと受講する前は思っていたのですが、一つ一つの説明が詳細で深い理解に繋がりやすかったなと思いました。非常にわかりやすい解説をありがとうございました。次回以降もよろしくお願いいたします。', '言語モデルが巨大化している理由として、巨大化すればするほど性能が上昇することが予想されていることや、モデルサイズが巨大でないと解けないタスク（mod計算など）が存在することがわかったことが挙げられていたのが興味深かった。', '各回の概要を比較的詳しく説明いただけたこと。', '専門用語もかなり分かりやすく説明していただいた。', '今後も非常に楽しみです。', '基礎の整理を効率的にできた点', 'すぐ講義に入るのではなく、オリエンテーションが1回目にありとても分かりやすかったです。', 'かなり幅広い部分を丁寧に教えてくださった。\\\\nURLのリンクを多く入れてくださり、情報の元にたどること、そして授業外での自己学習が容易になるようにしてくださったこと', 'なし', 'GPT-3, GPT-4…、と生成AIの製品が更新されるごとに、具体的に何が変わっているのか？\\\\nより「良い」言語モデルとはどのような指標で測られて「良い」とされているのか？\\\\nなぜ日本語より英語で書いたプロンプトの方が良い解答を得られることが多いのか？\\\\n等々の、これまでぼんやりと持っていた疑問について、解像度の高い答えを得るためのヒントとなる情報が散りばめられていたこと。', '有名なLLMは知っていることもあったが，特定のドメインに特化させたLLMについては細かく把握していなかったので，知れて良かったです（FinGPTやMed-PaLMなど）\\\\n個人的には，ドメインとそのドメインにおける代表的なLLMに関する表などあると嬉しいなと感じました', 'LLMの三つのフェーズ（事前学習、事後学習、RLHF）の詳細な説明と、それぞれのフェーズでどのような技術や方法が用いられているかを具体的に学べた点。例えば、事前学習における「次の単語予測」の仕組みや、ファインチューニングでの「LoRA」技術の効率的な応用方法についての解説が非常に分かりやすかった。また、RLHFの必要性や具体的な適用例（例えば、有害な出力を防ぐ学習）の説明も実践的で興味深かった。これらの知識は、LLMの構築や応用において重要な基盤となると感じた。', 'RLHFの学習方法の具体例として提示された「DPO」や「reward model」についての詳細な説明が不足していたと感じる', '最新の技術トレンドや研究内容について積極的に紹介し、受講者に興味を持たせようとする姿勢が素晴らしかった。また、具体例を用いて複雑な概念を分かりやすく説明しようとする努力が感じられ、技術的な内容を理解しやすかった。', '講師の知識が豊富で、プレゼンテーションも非常にわかりやすかったです。引き続き、AIの最先端技術に関する新しい知識が得られる講義を期待しています。ありがとうございます。\\\\n\\\\n今回の講義で興味深かった点として、医療ライセンス試験に「禁忌肢」という選んではいけない選択肢があること、そしてAIがその選択を誤って選ぶことがあるという話がありました。これは将来的に、AIと人間の回答を区別するための仕組みとして利用されるのではないかと考えました。\\\\n\\\\nまた、AIは学習を重ねることで賢くなり、より多くの問題に正しく回答できるようになりますが、そのプロセスは人間の学習とも通じるものがあるように感じました。ただし、AIには人間の「勘」のような直感的な判断力が欠けているのではないか、という点が気になりました。人間の「勘」は、長年の経験や潜在的なパターン認識能力によって培われるもので、AIがこれを再現するためには異なる学習手法が必要なのかもしれません。場の空気感を理解するための「身体性」を有することで、AIも人間のような勘を持つことができるのではないかと思います。', 'なぜ他ユーザと同じ質問をしてはいけないのかわからなかった。回答の速度は早く良いと思った。参考文献も付いていたので信頼できた。', '今後の講義の内容についてサラッと流すのではなく、一つ一つ割とじっくりめに教えてもらえてよかったです。', '大規模言語モデルの成り立ちや特性を知ることができた。', 'この講座の意義の部分について丁寧に説明があり、より意欲が湧いた。', '２時間連続は集中力の面で厳しいので、途中で2,3分程度でいいので休憩が欲しいと思った。', '丁寧に説明されていて良かった。初学者にも伝わるようにできる限り簡単な言葉で説明されているのも良かった。', '各回の概要がそのままLLM技術のOverviewのようになっており、体系的に理解ができました。', '使用した文献がリンクとして提示されるのが非常によいと思いました。講義終了後も論文サーベイ等に使ってみたいです。', 'スケーリング則の重要性について再三説明していただいたところです。', '今後の講義も非常に楽しみにしています。', 'これからLLMに関する情報を取得できるのはすごく楽しみですごく好奇心が湧きました', 'ニューラル言語モデルや元となる条件付確率の作用を詳しく学べたことが非常によかった。', 'LLMを今学ぶべき理由について詳しく知れた。', '聞き取りやすく細かく説明してくださった。', 'もう少しテンポが早いと嬉しいです。重要なところ以外は要約して貰えると嬉しいです。', 'これからの講義のすべての概要が分かったので、ワクワクした。\\\\nむずかしそうではあるけれど、内容の全体像が分かったので、学ぶ意欲につながったと思う。', '細かい専門用語は、初学者にとっては、分からない場合もあると感じた。\\\\n私はある程度、独学で学んでいるので分からない点は少なかった。\\\\n⇒『よく使う専門用語集』があると躓きにくいと思った。\\\\n⇒『各回ごとのよくあるQ&A』などを、過去振り返って作成し、さらに毎期ストックしていくと学習者にとって有難い。\\\\n', 'とても聞きやすくて、内容が伝わりやすかった。', '無料で、松尾A研究室の講義を聞くことができることに、感謝です。\\\\n\\\\n成果につながるように、積極的に学びたい気持ちでいっぱいです。\\\\n外部の小菅講師の内容も楽しみです。', 'Tanukiの開発状況がよくわかり、日本もまだまだこれから勝てる位置にいるのではないかと希望が持てました。\\\\n自身もLLMについてよく学び、役立てることがあれば協力したいと思いました。', '全体的にわかりやすいご説明でした。', '全体的に解説がわかりやすく、話すスピード、トーン、音質などが良く、しっかり理解できました。', '講義そのものではないのですが、受講の手引き、メール、Omnicampus、Slackと情報が散らばっており、効率の悪さを感じました。\\\\n（ものすごく改善して欲しい！というわけではないのですが。。。）', 'チャットbotにどこからアクセスできるのかがわからず、利用できていないです。申し訳ございません。', 'これからの1回1回の講義で何を意識して学べばよいかつかめたこと。最新の日本のLLM研究動向をうっすらと知れたこと。', 'LLM実装の現状と今後の小型化への展望について経済的コストの観点からもう少し知りたい。', '論文データなどの客観的なデータを基にして、LLMの概要について初学者でもわかりやすく説明されていて、集中して取り組むことができた。', '初回授業として基礎部分やそれに伴う研究、また最近の研究などを講義してくれたため、導入としてとても惹かれ面白かった。', 'どの講師の方の説明もとても分かりやすく、頭にすんなりと入った。', '概要を説明していただいたので来週以降のイメージが具体的にわいてきてよかったです。', '非常にわかりやすかったです。', '全体的に満足でした。', '週２で開講してほしいぐらいには面白い内容でした', '講義前半のLLMの概要から応用例の説明があってから、各回の講義の概要について説明があったので理解がしやすかった。', 'LLMの概要の説明時にコードで実装するときにどのような形式で書かれるかの説明があればコードを各イメージがしやすかったと思います。', 'それそれの説明の長さが長すぎることがなく理解しやすかった。', '講義動画のアーカイブは今後作られるとのことですが、資料の閲覧はどのようにすればいいのでしょうか？', '後半の解説', '講座の概要を話していただきありがとうございました。今後の学習の見通しが良くなりました。', '大規模モデル全般の説明が関連研究を用いて簡潔に行われ，これまで独学で学んだ大規模モデルの仕組みやRAGに加え，新たな学びができてよかったと思いました．また，講義内容とスライドが洗練されていて，各回の概要の紹介スライドのデザインがよく，読みやすく感じました．初回とは思えないぐらいの学習量であり，すごく楽しかったです．', '不満はございません．説明がすごくわかりやすかったです．今後の講義楽しみにしております．', '今まで自分が理解した気になっていた部分を解説して貰えた点', 'もうすこし、声の音量を上げて欲しい', 'chatGPTなど身近な例が多かったのでわかりやすかったです。', '特にありません。', '全体的に説明がわかりやすくて、丁寧で、理解できました。', '去年の修了者が3,4割と聞き、課題等が難しいのか少し不安になりました。ただ、先生方が丁寧に説明してくださってとてもわかりやすいので修了できるよう頑張りたいと思います。よろしくお願いいたします。', '参考文献の論文が非常に多く示されており，深く知りたいと思った項目や自身の専門と関連のある内容について調べることが容易だったから．', 'ＬＬＭの構築方法はわかっていなかったので、よかったです。', 'これからの各講義回の概要を俯瞰することができたこと。第10回：LLMの分析と理論が気になっています。第9回までに学んできた技術の根幹について', '初回ということで特にありません。これからたくさん出てくると思います。', 'A講師、B講師、どちらも簡潔な説明、聞き取りやすい口調で分かりやすかったです。', '引き続きよろしくお願いいたします。', 'チャットボットを活用してみます。何か気づきがありましたら、フィードバックさせていただきます。', '最初に初心者のための軽い解説、その後個々の講義概要を説明してくれたため、今後の講義で、復習しやすいと感じた。大変ありがたい。', '特になし。', '良い声でした', 'とてもわかりやすい講義でした。ありがとうございました。', '基礎の基礎から教えてくださったことで、良い復習になったし、専門学校の先生が教えてくれないLLM全体のとらえ方がわかってとても良かった。', '特になし', 'A先生もB先生も簡潔にわかりやすく話してくださったのでとても良かったです。', '今後の講義の全体像についても教えていただき明確にイメージができた。', 'この度はこのような機会を頂きありがとうございました。', '概要が分かり今後の講義が楽しみになった。特にLLMの５回のfine tuningが気になる。また個人的にチューニングしたモデルへ強化学習を行う方法方法も気なるため7回も楽しみとなった。', '事前にTransformreを触っている自分には、一回目の概要としてこの講義内容は適切な難易度だと感じた。', 'LLMの発展の流れやNLPを研究する上で特に理解をすべき項目が各回のメインテーマになっていたので、自分自身の学習の方向性がイメージがしやすくなった。', '講義回や質問範囲で質問をソートできると講義に直接関係ある内容を確認しやすくなると思いました。', '各回でどんなことを学ぶのか、既に知っている部分や聞いたことがある程度の箇所など、学習イメージがついたことが良かった。', 'LLM講座で今後学習する内容だけではなく、学習する内容を軽く説明してくれて、今後学ぶことをイメージできました。', '半導体に関してはどの程度の知見を持っておくべきなのか、少し気になります。', 'アメリカテック企業と比較して、日本発のLLMの現在のモデルサイズは2019年程度みたいですが、軽量なモデルがなぜGPT-3.5と同等以上の性能を持つのでしょうか。Tanukiに限らず、NECのcotomi(モデルサイズは非公開)、NTTのtsuzumiも似たような結果になっています。日本語に対する評価値で比較しているからなのか、学習しているデータの前処理が異なるのか、なぜでしょうか。', '実際に開発された「Tanuki」についての講義も併催されており、これからも楽しく学習していきたいです。', '（当然かもしれないが）タイトルだけでは分かりにくかった第2回以降の講義の内容がしれてよかった。', '後半部分。', '夜のこの時間で2時間は脳みそのスタミナが切れていきますね..、2時間やるなら間に5分だけでも休憩があるとより良いなと思いました。', '今回の講義で，全体観がわかったので，次回以降の講義が楽しみになった．', 'スライドが分かりやすかった。', '特になし。', '特になし。', '本日の講義で特に良かった点は、大規模言語モデルに関する説明が時系列に沿って整理されており、非常に分かりやすく、かつ情報量が豊富だったことです。また、LLM講座全体の構成についても具体的に説明されていたため、今後の学習の進め方がより明確にイメージできました。', '特になし', '特になし', '質問及び回答のレベルを概略的に表記しても良さそうな気がします。（LeetCodeのeasy, medium, hardのように）', '各講義を聞く前に、そのモチベーションとなるようなお話を最初にお聞きできたことは、今後の講義を受講する上でよかったと思います。', '今回の講義では、LLMの原理や、今LLMを学ぶ理由について知ることができ、良かったです。また、各回の概要を聞くことできたため、今後、それぞれの回の目標や講義全体としての到達点を意識しながら学習できる点が特に良かったと思いました。', '一つ一つの説明が丁寧で、これから学ぶ内容が頭の中で整理されました。', '本講座を通して、大規模言語モデルについての理解を深め、AIに関する技術力をさらに高めていきたいと思います。', '講座の全体感を丁寧に説明していただけるのは自分のスキルに対する難易度等がイメージ出来て良かったです。', 'LLMの基礎を解説いただいたことで、プロンプト作成業務にも活かせそう。', '将来（松尾研究室のことや、日本におけるAI分野にも将来性があること）を知れて、興味深く感じた。', '最近の動向を短時間で教えて頂けたことが非常に有難いです。', '本講義の全体像について説明してくださったおかげで、講義に対するモチベーションが上がりました。', '大規模言語モデルの概論を知ることが出来た。', 'LLMの全体像を概観出来て良かったです。', '平易な言葉で解説頂けたのが良かったです。', '今後の講義予定を聞いてわくわくしています。', \"Let's think step by stepは出力にあたるAnswerの冒頭に入れることを始めて認識した。\", '各講義を受講すると、何が身につくのか、見通しをもたせていただきました。', '特に、なぜLLMを学ぶのか？という観点で、意義を含めて背景、前提を理解することができた点が良かったと感じました。\\\\nまた、今後の講義についても一度俯瞰して把握しておくことで、イメージが湧くだけでなく、事前の学習にも役立つというメリットもあると感じています。', 'LLMの基本概念から最新の応用例まで、幅広くかつ体系的に説明されていた点がよかったです。\\\\n特に、なぜ今言語モデルが注目されているのかという観点から、モデルの巨大化、Promptingによる汎用性、他領域への影響という3つの要因を挙げて解説されていた部分が印象的でした。\\\\nまた、具体的な事例や図表を交えて説明されていたので、理解が深まりました。', '講義全体の流れが分かりやすかった', '日本のLLM 開発環境について、特に具体的な計算リソースの数値や、データセットの実際のサイズが示されたことで、現状が把握しやすかったです。', '歴史的な流れを踏まえた解説や、実際の LLM 開発経験に基づいた解説が興味深かったです!', 'ポイントがまとまっており、なおかつ専門用語が少なかったので分かりやすく聞くことが出来ました。', 'とくにありません。', 'お話が大変分かりやすく、音質も明瞭でとても助かりました。ありがとうございました。', '後悔しないようにがんばります！！！', 'チャットボットの仕組みもどこかに書いてあると嬉しいです。', '今年の講義でどのような内容のものを学べるかの詳細を説明いただいてだいぶイメージがわきましたので、よかったと思います。', 'とくにありません。', '講義の流れが把握でき、受講イメージがわきました。', '特にございません。', '前回同様わかりやすかったです。', 'よろしくお願いいたします。', 'Vision 系や音声との接続、エージェント系の話が少し昨年より（今回の内容は概論レベルではあるので今後出てくるのかもしれませんが）減ってしまった感があるので、もう少し触れて貰えると嬉しいです。', '喋る速度やしゃべり方はとても聞きやすかったです。', 'たいへんな講座を実施していただきまして大変感謝しております。LLM講座も２年目ということで、内容も構成もやり方も全て手探りの中、駒場の900番講堂にすら入りきらない非常に多くの人数を相手に講義をする苦労は全く想像が及びませんが、大変な苦労をなさっていることは分かりますし、本当に感謝しております。教えて頂いた内容を元に少しでも社会実装を進めて行ければと思っております。よろしくお願いいたします。', 'LMを学ぶ意義と現状および各講義の全体像が理解できた。', '特にありません。', '特にありません。', '講義の準備など大変なことも多いかと存じますが、楽しみにしておりますので、今後ともどうぞよろしくお願いいたします。', '特にありません。', 'RLHFを使ったLLMの制御手法について、具体的な例を用いて説明されていた点が分かりやすく、実際のモデル調整の方法を学ぶ上で非常に役立ちました。', '計算リソースとモデルパラメータ数の関係について、もう少し知りたかった。', '現在の対話はスムーズですが、特定の質問に対する回答が少し一般的すぎる場合があります。', '講義の全体像を1日で学ぶことができた。なぜ学ぶのかが理解できたしアカデミックとインダストリーの間で非常にバランスの良い講義だと思いました。', '特にありませんが、オーバービューの講義だと思うので理解しきれない内容が多くありましした。', '自己紹介でバックグラウンドを教えていただけると理解が進みました。', '本日はありがとうございました。講義についていきながら最後まで完走したいと思います。\\\\nどうぞ、よろしくお願いします。', '自分の名前がリストで表示されない方がより使ってもらえると思います。', 'スケーリング則の３つのチャートで、横軸が計算量になっているものは理解できるのですが、残り２つの、横軸がデータ量とパラメータ数になっているものは、軸にないパラメータ（例えば縦軸がロス、横軸がデータ量のときはパラメータ数）がどのように処理されているのか（固定されている？）が理解できていません。', '今回もまたよろしくお願いいたします！', 'モデルのサイズ（と精度）のスケーリング則から，今後登場する言語モデルのデータと計算量規模がおおよそ予測でき，近い将来のおおよその見積もりを提示してくれているところが役立った．「なぜ，言語モデルを学ぶのか」の解説でも，過去のNN機械学習によってできたことと，現在の汎用的な機械学習との違いを比較されていて，パラダイム・シフトが起きていることがよく分かり，その後のロボットへの応用などの解説につなげているので．ストーリがわかりやすかった．', 'LLMの歴史や現在の状況などを行ったり来たりしてくださったこと、さまざまなモデルについて触れてくださったので、今後の情報収集のきっかけになると思いました。', '仕方のないことなのですが、論文引用の文章が英語のままだったので、その場では理解が及びませんでした。引用文献が書いてあったりリンクが貼ってあるものは後から追えるのですが、スライドに書いていないものがありましたので、リストなどがあると大変ありがたいです。', '惜しみなく情報を開示くださったこと、講師の先生方の言葉で解釈して説明くださるので、書籍を読むのとは断然の差で、わかりやすかったです。', 'ついていけるかドキドキしています。が、今時点でしっかりキャッチアップすることが次に繋がる予感がしますので、頑張ります。', 'とてもわかりやすかったです。', '特になし', 'ありがとうございましtrain。', 'まだ使っておりません。', '第2回以降の個別の技術紹介をしていただく前に、重要なポイントやそれらの位置づけ、各トピックスの概要などを紹介していただき、展望や現在我々が置かれている状況が分かり良かった。', '特にありません。', '他の参加者の疑問とそれに対する回答がとても参考になります。ただ、個別の回答からリストに戻る時にリストの元のページではなく最初のページに戻るのが使いにくいように思います。また、botの回答が質問の論点とずれている場合に、人間による回答をお願いできるチェック欄が有った方が良いかもしれません。', '今後の概要がわかったので、できる範囲で予習したいと思います。', 'このような素晴らしい機会を与えていただきありがとうございます。', 'chatGPTに同じ質問をしましたが、chatGPTの方が素人にわかりやすい回答でした。', '講座に参加できるようになったとき（採用）のメールを、後で検索したときに見つかりづらく、メールのタイトルに松尾研LLMなどの思いつきやすいキーワードがある方が見つけやすいと思った。', '学習データ収集時の注意点など実際にLLM開発を行う際に必要なポイントが説明されており、とても良いと思いました。', '昨年もLLM講座を受講しましたが、講座の内容がアップデートされておりますので、これからの講義をとても楽しみにしています。また、学習データ収集時の注意点の際に説明があった著作権については、可能であれば文科省の見解を参考にさらに詳細な説明をしていただければありがたいです。講義1回分説明できるくらいの内容だと思いました。さらに、JGLUE、JMTなど評価ベンチマークについても今後の講義で取り上げていただけるとありがたいです。今回の講座を受講するとLLMの実装をできるようになりますが、評価ベンチマークの作成もできるようになりたいと思っています。', '私自身は質問していないですが、ほかの方の質問を見ると「https://edu.omnicamp.us/courses/65/llm-qas/196/」が引用元として多いなという印象を受けました。他の引用元より優先して選択されるようになっている可能性があると思いました。', '昨年と比べて内容・ボリュームともに拡大しており、わくわくしています。', 'なし', 'LLMの全体像をざっくり解説していただいたおかげで、今後の講義の各回が全体像のどの部分に当たるかを意識しながら学習できるため、より理解が深まると思います。', '昨年も受講しましたが、その時にはなかったカリキュラムもあり、（特に、半導体、Domain Specific、Controlなどは）体系的に学べる機会もなかなかないのでとても楽しみです。', '本講座で学べること，目指せる姿がイメージしやすいように，講座の概要が示されていた点．', 'LLMの応用に関して，具体例について説明することはできないため，重要な点をピックアップして，内容が今よりも少なくても良いと感じた．', '他の講義受講者の質問も確認したいがすべてを確認することは無理なので，テーマごとに分類したり，似たような質問を要約したページが定期的に（例えば1週間）アップデートされたりすると嬉しい．', 'p.7 にて、「条件付き確率がわかると生成することができる」とあり、\\\\n例では、「日本の首都は」に続く単語として、「東京」「パリ」...「カイロ」などの単語が続く場合の条件付き確率を求めていますが、\\\\n「日本の首都は」に続く単語をどのようにして選ぶのかが分かりませんでした。\\\\nBruteforceで辞書にあるすべての単語で条件付き確率を求めるのでしょうか？\\\\n', '質問をしたところ、回答がすぐ返ってきたので素晴らしいと感じました。本Chatbotについても講義の中で作り方とか構成など紹介いただきたい。', '全体像がつかめてよかった', '第10回のLLMの理論の回が特に楽しみです', '各回答の正確さがどれぐらいだと思って読んだらいいのかが分かると嬉しい（100%かどうかという意味ではなく、20%なのか70%なのかぐらいの感じでもわかるとありがたいです）', '23年からの様々な状況変化を踏まえたアップデートがされており分かりやすかったです。', 'geniacプロジェクトで作成されたtanukiモデルの紹介があったのが良かったと思いました。', '講座のが、前半（基礎的な技術）と後半（応用）として構成されており、より理解しやすかったです。', '特にありません。', '特にありません。', '本年度も楽しみにしております。よろしくお願いいたします。', '第1回では使用しなかったため、今後利用の機会に気付くことがありましたらフィードバックさせて頂きます。', '昨年からさらに進んだ点について簡潔にお話くださったので、現状が伝わってきました。', '大変良く理解できたと思います。', '淡々と噛み砕いてお話くださったので、講義に集中できましたし、よくわかりました。', '本日もありがとうございました。講義を生かせるように努めます。', '昨年度の講義資料自体が素晴らしいと思ったので１年経過しても改めて勉強になりました。', '特にありません。', '昨年度同様大変分かりやすく、聴きやすかったたです。', '本年度も受講の機会をいただきmakotoni \\\\nありがとうございます。\\\\n何卒よろしくお願いいたします。', '新たに追加された講義部分を含め、前回に増して非常に興味をそそられるとともに、直ちに活用できそうな有用な情報が満載であることが実感できました。これからの学習（座学と実習）に対する自らのモチベーション、率直にわくわく感がとても向上しました。', '特にありません。', 'A先生の長年のご研究に基づく大所高所からのご説明は受講生の視点の引き上げにとてもよかったと思いました。ただ少し高すぎて、一緒に参加した初学者の方がお分かりになったかを少し心配していました。一方で、大局的なスタイルのB先生のより実践的なご説明は、これからの講義のいい予備知識となりました。', '今回も、学生のみならず企業に門戸を開いていただきありがとうございました。会社の若手を誘って参加させていただきます。一方で、やはり学生さんの育成は日本の将来にとって重要であり、今回、近親の学生さんもお誘いして併せて参加させていただきます。', 'とても分かりやすかったです。ありがとうございました。', '日本の現状が聞けたこと', '特にありません。', '特に不満はありません。', '昨年も参加させていただきました。今年もアップデートと新たな単元が加わり、今から受講を楽しみにしています。', 'LLMの概要についてザックリと知ることができたこと。自分で情報を得ようとするとどうしても漏れや偏りが生じてしまうため、こういう概要は大変助かる。', '各回の講義について説明していただきましたが、内容が充実しており次回以降の講義を楽しみにしています。\\\\n特に半導体などのハードウェアに関する内容は興味があり期待しています。', '気になった点：アクセスに数秒のロードが発生する点、Botの回答の正確性（講師陣の回答との差分、講義の流れに応じた回答になっているか）', '全体感をうまく示していただき、とてもわかりやすかったです。', 'お二人とも説明がポイントをついておりとてもわかりやすかったです。', '日本発LLMの重要性、必要性についてもどこかでお聞きできればと思っています（アーカイブ受講のため質問できませんでした）', 'イメージがつかめたことです', 'これからは、具体的な内容に入っていくかと思いますが、分かりやすく教えて頂けると非常に助かります。', 'フレンドリーな感じで講義が聞きやすかったです。', '今のところは、ありません。', '講師の方の資料の作り方が非常に丁寧でわかりやすく、また、口頭での説明も声量がきちんとあり聞き取りやすかったので、ストレスなく視聴できとても助かりました。さらに、LLMについてほとんど知識がない初学者を置いていかないような説明で大変良かったです。ありがとうございました。', '大まかな概要がつかめた', '難しすぎてついていけないのでは、と思っていましたが、今回のガイダンスで難しいけれども頑張ろうと思えました。なんとか喰らいついて行きます。', 'LLMの周辺知識について知ることができた、初耳なワードばかりだったので予習しながら少しずつ知識を蓄積していく必要性がよくわかった。', 'slackを講座のものとコミュニティのものに分けてくださると情報が多くならずにわかりやすいと思います。\\\\n例「GCI LLM講座 2024」と「GCI LLMコミュニティ」に分け、2024に入った人のみ前者に入る', '本講義を受講させていただくことができ、とても光栄です。大規模言語モデルについて、知識を学び、各回の宿題や最終のコンペにて実践的な力も同時に鍛えられるよう、努力してまいりたいと思います。よろしくお願いいたします。']\n",
      "negative comments:['OmniCampusにZoomのリンクを載せる動線では駄目でしょうか？_毎回、動線が散漫としていてそれが本当に億劫です。', '昨年資料にもありましたが、第１回目で各回の紹介は要らないように思います。', 'ぜひ英語表記を併記してほしいです！日本語表記になると訳文だとわかる日本語になってしまい、本当に引用文献中の正しいことを言っているのか正誤判断が煩雑になります。また、後ろでRAGを使われていることと思いますが、資料で回答できないことはわからない、と言い切った方が良いように思います。結構はルーシネーションしているように見受けられるので、、', '各回の概要について。全体像を掴めたのは良かったが、1時間超かけてやるべきかは議論の余地があるなと感じた。', '学習器は後で構成を学ぶなら、図を出すのではなく、その図の思想や概念を出してほしい。', '各講義の概要は各講義で学ぶので簡単な頭出しにとどめても良かったかもしれません。触りにしては少し時間を使いすぎている感じがありました。', '２回目以降の内容がついていけるか不安なので、もう少し内容を掘り下げても良かったような気がします。', 'ついていけるかすごく不安ですが、頑張ってついていこうと思います！', '話し方が単調だったので、もう少しメリハリのある話し方であるとありがたい。ただ、内容重視であると思っているので、そこまで強い要望はない。', '事前資料をもう少し早めに提示していただきたく思います。100ページはさすがに2時間で読み通すのが難しかった。→2回目以降分もすでにアップされているので問題ないです。\\\\n質問対応を1としたのは質問対応が発生しなかったためなんとも言えないからで、悪い評価をしたということではないです。', '資料投影画面を録画教材として欲しいです。講師画像では次回以降内容が具体化する場合、内容の把握が難しいです。', '初めて講座を受講したため、講座の基本的な枠組み、全体的な概要を説明いただいたことは親切であると感じました。', '計算量の所がさらりと流されてしまったので、もっと詳しく知りたかったです。\\\\n事前学習、事後学習にかかる時間や資源（GPU、メモリ）の計算式を理解したいです。', 'LLMを専門とする職務を担当しておりますが、お恥ずかしながら基盤となる技術に関する知識が確立しておりません。今後LLMの仕組みや構築方法、そして業界への応用の事例に関して詳しく学習できることを楽しみにしております。', '概要パートは、結局後でやるのだから説明は必要ないと感じた。概要を元に受ける授業を選ぶわけでもないので、この生成AI時代にどう立ち向かっていけばよいかなど、ビジネス的な部分にも踏み込んで話してくれるとありがたかった', 'LLMの学習にリソースが足りない場合の対処方法に興味があるので、並列化の方法の講義を楽しみにしています。', '多様なタスクへの回答が実現できるような仕組みはよくわかりました．その一方で金融，医療，法律，コーディングのような，特定の領域や技能の知識を持つように訓練されたモデルも存在し，分野に特化したモデルの方が需要がでてきそうにも思います．その際に，いちいちモデルを作るなどは大変で，ファインチューニングでどこまで対応可能なのか，今後の講義内容を期待しています．', '日本LLMの現状について教えてもらえたのがよかったです。', '次回以降の講義、期待しております。', 'RAGについてあいまいな理解しかなかったのですが、スライド1枚で非常にわかりやすくまとまっていたものがあって勉強になりました。\\\\nまた、半導体についての話や、今後の講義で分野特化のLLMやロボットへの応用方法についても学べるということで楽しみになりました。', '私自身がLLMについての全くの初学者であるゆえに、ついていけない部分が多くあった。とはいえこれは予備知識に不足のある自分の非であり、なんとか分からないなりに吸収できるものがあればと思って講義に臨んでいるが、果たして自分に修了できるかという不安が残った。', 'AIブートキャンプのDS演習も同時にやっているので、良い復習になった。', '前提知識が不足していたため、それを補う事前資料などがあれば良いかなと思いました。', '何分か前から配信自体は始まるかなと勝手に予想していたせいで、なにか設定ミスかなと焦ってしまった', '初回なのでまだ勝手がわかっていませんが、2時間盛りだくさんだったので休憩が途中とれると良いと思いました', 'あまりにも多くの質問で溢れており、見づらいです。そのため、いいね機能などを追加していいね数順にソートをするなどして、特に有用だった質問が上に上がってくるようにして欲しいです。', '資料中と最後のReferenceの番号が一致していない', 'すでに多くの投稿がされているので、類似質問を探すのが困難になっています。キーワード検索ができるようになると、より類似質問が検索しやすくなるのではと思いました。', 'Scaling Lawの図がどのようなデータをもとに算出されたのかをもう少し具体的に理解したかったです。', '多くの概観を知れてよかった\\\\n各論の講義が楽しみです。', 'チャットbotは使っていません\\u3000表示されているのは分かりました覚えています', 'スライドだけでは理解が難しい部分を動画等でわかりやすく説明してくれたところ\\\\nex) figure AI', '講座を開講いただきありがとうございます。一生懸命勉強させていただきます！', '変化が激しいLLM分野で、データの整理、傾向分析・プレゼンテーションおよび噛み砕いた説明が良かった', '最新のLLMの技術が外観できてよかった．トレーニングにどれくらいの計算量が必要か実感できた．GPT4などのLLM作成のためにどれだけのGPUを用いてどれくらいの時間で学習しているのか実感できた．学習データの量が図書館のデータ量と比較して出てきて，実感が分かった．', 'Slackがチャンネルの多さと情報量の多さで扱いにくいです。', '私自身、聴覚障がいを抱えており、本講義につきましては別の音声認識アプリを用いて文字起こしをしつつ視聴させていただきました。しかし、それではPPTと同時に見ることができず話す内容に追いつくことが出来ませんでした。もしよろしければ、Zoomの文字起こし機能を使って字幕を提示していただくことは可能でしょうか。よろしくお願いいたします。', 'これからLLMの学習が進められるか不安と期待が半々です。', '合っているかを確認できるよう、根拠を示すようにして欲しい。（Promptingで解決できる？）', 'スムーズな進行でとってもわかりやすくて良かったです。', 'ZoomのURLを返せていなかったのでそこは返してほしいなと思いました。個人的な感覚では今のbotは当たり前品質がカバーできてないように映ります', '出欠アンケートの一時自動保存などあると助かると思いました。途中で消えるのを避けるため。', '前半の概要は2023の講義で話していた内容と大きく被っていたので、2023との差分をわかるように示していただけると、安心して聞くことができたと思う。（強いて言うのであれば、という程度です。ありがとうございます。）', 'tanukiの開発時の話はとても興味がありますが、個人的な都合により参加できるかわからないので、アーカイブ等で配信していただけると嬉しいです。', '途中、3-4分の休憩があると助かります。トイレが近いので。\\\\n5段階の評価の指針を提示していただくと、より正確な評価ができるとおもいます。\\\\nまた、今回は質疑応答が無かったので、質問に対する回答の評価ができませんでした。該当しない項目の評価についても指針を示していただけると助かります。', '全体に表示されない、個人用の非公開のチャットbotが欲しいです', 'B先生の声の音量がもう少し大きい方がありがたかった。', '個人的には内容が多いと思った。2時間集中するのはきつそうなので、今後は前半1時間をリアルタイムで、後半は録画で学ぶことになるかと思う。', 'すこし時間が長かった。', 'ハードウエアに関連して、並列演算や論理構成などの分野から、LLM\\\\nの圧縮手法などに話題を広げてしまうと、初学者には厳しいかもしれない。', '声が聞き取りやすく、説明も平易であったため、事前知識が少ない状態でも理解しながら講義を受講することができました。', 'LLMの学習が3段階に分かれている事、それらの意味合いを人間が学習する場合（義務教育など）に例えてご説明いただいた。\\\\n今後、講義のレベルが高くなるにつれて数式の割合が増えていくかと思いますが、アナロジー的な表現を継続頂けると理解が進み大変助かります。', '特になし。', 'https://edu.omnicamp.us/courses/65/llm-qas/164/\\u3000「使用した参考文献」が不要な場合は非表示で良いと思われます', '特にないですが、ディープラーニングの初学者の方が参加されていた場合、いきなりCNNやRNN、MLPや尤度の話が出ても何のことか分からないのではないかと思いました。', '前半のLLM学習量のお話で、トークン数の例として国会図書館や東大図書館の書籍数との比較をされていたのが、どの程度の学習量なのかが理解でき、良かった。', '今まで知りえなかった基本的なことが知れたので良かった。', 'まずは全体像をしっかりお伝えいただけた点が大変よかった。', '大まかにですが、LLMの基本的な仕組みと、開発に必要なステップを理解することができ、完全にブラックボックスだった技術の解像度を上げることができました。\\\\n各項目について自身で調査や学習することができるようになったこともよかったです。', '第5回のSupervised Fine-Tuningの説明は理解できなかった。', '自分の目的は、休まず講義を受講して修了証をゲットしたいと考えております。コンペで上位を狙うつもりはありませんが、ファインチューニングの個人演習が不安で心配の気持ち（パイソンでケラスデータセットのCIFAR10や100を使ってCNNの畳み込みニューラルを１から記述するとかですと、いまの自分のレベルですと今からかなり準備が必要と感じています）でいっぱいです、、、自分は、人よりも、のろまなので、今からファインチューニングの勉強をして修了証を取れるように、落第しないようにがんばるには、どうしたらよいのか、分からない状況で困っています。', '・はじめに今後の講義でどんなことを学ぶのかを解説して頂けたので、今後の講義に向けた準備に役立つと思いました。\\\\n・話すスピードが心地よく、頭に入ってきやすかったです。', '情報量が多く、ついていくので精一杯でしたが、わかりやすい質問のおかげでなんとか理解ができました。', '本講義の社会人は40％とのことでしたが、そのうちわけ（業種、開発職、営業、行政職員など）の割合等も参考までに知りたいです。', '全体が俯瞰して理解できた', 'スケーリング則の説明など、引用されている論文の図表の読み方がよくわからなかった', '本講義を通じて、それぞれの技術がどの程度理解を深められるのか、先生方がそれぞれの技術をどのぐらい重要であると捉えているかという視点をもう少し知りたかった。', '文字数制限の状況やパラメーターの使用方法がわかるとありがたいです。', '出欠アンケートの授業評価ボタンが小さくて押しにくいのでもう少し大きくていいと思います。', 'コースの目的や各回ごとの内容を示してくれた点', 'これからの講義がより楽しみになった。', '声が聞き取りずらい部分があるのでマイクの音質を上げてほしい。', '文章が完全に一致していない場合でも検索することができると良いと思いました。', 'ファインチューニングなど、先の学習内容は理解しにくかった。', '１コマ２時間の講義はかなり長く感じた。アーカイブ視聴のため、何度か中断して行った。この３年ぐらいの進化の凄さも感じられた。', '教材の保存場所（googleドライブのアドレスを聞く意図）を聞いたところ、少し的外れな回答でした。受講者の前提条件（LLM2024の講座を受けており、こういう情報をインプット済）の情報があると、精度が良くなりそうな印象を受けました。', 'LLMの評価指標について、具体的なタスク内容の説明があれば、評価方法のイメージが掴みやすかったと思います。', '日本語LLMの開発状況リストや、LLMの推論ハードウェア開発など、昨年度講義に含まれていなかった部分の講義が特に良かった。', 'RAGがパイプラインだという認識がなかったので、RAGについてしっかり勉強したいという意欲がわきました。', 'LLMの評価指標については未だ理解が浅く、どのような基準や指標で評価しているのか不明なので、後日の講義でしっかり勉強したいと思います。', '特になし', '本来であれば難解なことを判り易く説明していただきました。プレゼン中の引用元URLが役に立ちます。']\n",
      "important_commnts:['OmniCampusにZoomのリンクを載せる動線では駄目でしょうか？_毎回、動線が散漫としていてそれが本当に億劫です。', '昨年資料にもありましたが、第１回目で各回の紹介は要らないように思います。', '各講義の概要は各講義で学ぶので簡単な頭出しにとどめても良かったかもしれません。触りにしては少し時間を使いすぎている感じがありました。', '事前資料をもう少し早めに提示していただきたく思います。100ページはさすがに2時間で読み通すのが難しかった。→2回目以降分もすでにアップされているので問題ないです。\\\\n質問対応を1としたのは質問対応が発生しなかったためなんとも言えないからで、悪い評価をしたということではないです。', '資料投影画面を録画教材として欲しいです。講師画像では次回以降内容が具体化する場合、内容の把握が難しいです。', '初めて講座を受講したため、講座の基本的な枠組み、全体的な概要を説明いただいたことは親切であると感じました。', 'LLMを専門とする職務を担当しておりますが、お恥ずかしながら基盤となる技術に関する知識が確立しておりません。今後LLMの仕組みや構築方法、そして業界への応用の事例に関して詳しく学習できることを楽しみにしております。', '多様なタスクへの回答が実現できるような仕組みはよくわかりました．その一方で金融，医療，法律，コーディングのような，特定の領域や技能の知識を持つように訓練されたモデルも存在し，分野に特化したモデルの方が需要がでてきそうにも思います．その際に，いちいちモデルを作るなどは大変で，ファインチューニングでどこまで対応可能なのか，今後の講義内容を期待しています．', '日本LLMの現状について教えてもらえたのがよかったです。', '延長なく終わったのがまずよかったです。\\\\n声とかも聞き取りやすかったです。', '次回以降の講義、期待しております。', '私自身がLLMについての全くの初学者であるゆえに、ついていけない部分が多くあった。とはいえこれは予備知識に不足のある自分の非であり、なんとか分からないなりに吸収できるものがあればと思って講義に臨んでいるが、果たして自分に修了できるかという不安が残った。', 'AIブートキャンプのDS演習も同時にやっているので、良い復習になった。', '前提知識が不足していたため、それを補う事前資料などがあれば良いかなと思いました。', '東大メタバース工学部の講義全般について感じていることですが、話のフックが弱い印象があり（平坦？）、初学者の私にとって難しく感じます。講義内容をアップデートされているとお話されていましたので、今後を楽しみにしています。', 'LLMに至るまでの歴史的な変遷は知らなかったのでためになった。', '喋り方、スピード、内容に関して不満はありません。', '講義を通じて、LLMの構造について具体的なイメージができたことで、これがどのように世界で必要とされ続ける技術であるかをより理解できました。特に、LLMが単なる言語処理だけでなく、様々な分野で応用可能な汎用性を持つことや、スケーラビリティによって解決できるタスクの幅が広がっていることが印象的でした。\\\\n\\\\nまた、TransformerのSelf-Attention機構が、言語の文脈を深く理解し、より正確な出力を可能にしている点も、LLMが他の技術とは一線を画す理由として納得できました。これらの理解を通じて、LLMがただの技術に留まらず、将来にわたって世界中で様々な問題解決に寄与し続ける存在であることを強く実感しました。', '仕方のない部分でもあり、もっと勉強をしておけよ。という観点もありますが、専門的な用語が比較的よく出てくるので、とっつきにくさはありました。', 'あまりにも多くの質問で溢れており、見づらいです。そのため、いいね機能などを追加していいね数順にソートをするなどして、特に有用だった質問が上に上がってくるようにして欲しいです。', 'すでに多くの投稿がされているので、類似質問を探すのが困難になっています。キーワード検索ができるようになると、より類似質問が検索しやすくなるのではと思いました。', '多くの概観を知れてよかった\\\\n各論の講義が楽しみです。', 'チャットbotは使っていません\\u3000表示されているのは分かりました覚えています', '数値や学術論文からのデータをエビデンスとして提示いただけたこと', '各回の概要を説明していただいたことにより、これからのカリキュラムのイメージができて講義に取り組みやすくなった。', '特になし', 'Slackがチャンネルの多さと情報量の多さで扱いにくいです。', '私自身、聴覚障がいを抱えており、本講義につきましては別の音声認識アプリを用いて文字起こしをしつつ視聴させていただきました。しかし、それではPPTと同時に見ることができず話す内容に追いつくことが出来ませんでした。もしよろしければ、Zoomの文字起こし機能を使って字幕を提示していただくことは可能でしょうか。よろしくお願いいたします。', '合っているかを確認できるよう、根拠を示すようにして欲しい。（Promptingで解決できる？）', 'ZoomのURLを返せていなかったのでそこは返してほしいなと思いました。個人的な感覚では今のbotは当たり前品質がカバーできてないように映ります', '出欠アンケートの一時自動保存などあると助かると思いました。途中で消えるのを避けるため。', '前半の概要は2023の講義で話していた内容と大きく被っていたので、2023との差分をわかるように示していただけると、安心して聞くことができたと思う。（強いて言うのであれば、という程度です。ありがとうございます。）', 'tanukiの開発時の話はとても興味がありますが、個人的な都合により参加できるかわからないので、アーカイブ等で配信していただけると嬉しいです。', 'もうちょっとマイクからの音声がクリアであるとよかった。※聞き取りには十分だったが、若干曇った感じになっていた。', '今後，各回で学ぶLLMについて体系的・俯瞰的に理解することが出来た点が良かったです．', '本日は特にありません', 'LLMについて、日本全体のレベル底上げを熱く取り組んでいるということが伝わってきました', 'わかりにくかった or 改善点というわけではないですが，ロボティクスの応用まわり（言語以外のドメインの話）の詳細は可能であればもう少し理解したかったです。', '途中、3-4分の休憩があると助かります。トイレが近いので。\\\\n5段階の評価の指針を提示していただくと、より正確な評価ができるとおもいます。\\\\nまた、今回は質疑応答が無かったので、質問に対する回答の評価ができませんでした。該当しない項目の評価についても指針を示していただけると助かります。', '個人的には内容が多いと思った。2時間集中するのはきつそうなので、今後は前半1時間をリアルタイムで、後半は録画で学ぶことになるかと思う。', '声が聞き取りやすく、説明も平易であったため、事前知識が少ない状態でも理解しながら講義を受講することができました。', 'まだ使っておりませんので要望はありません。', '・初回の講義で今後の講義で学ぶことの概要を俯瞰的に学べたことで、次回以降もゴールを見据えた目的意識を持って学習できそうと思えた\\\\n・LLMやそれを取り巻く環境、および関連技術の概要を学ぶことができて、LLMについて大まかな理解ができた', '次回からの講義が楽しみです。', '忘れていただけかもしれないが、Pre-train, Prompt, Predictという括りでの解説は新鮮だった。', '発話にフィラーもなく、淡々とお話されていた印象でしたが\\\\n内容も聞き取りやすく、分かりやすかったです。ありがとうございました。', '楽しかったです。また次回お願いします。最後のコンペ楽しみにしてます。', '特にないですが、ディープラーニングの初学者の方が参加されていた場合、いきなりCNNやRNN、MLPや尤度の話が出ても何のことか分からないのではないかと思いました。', '前半のLLM学習量のお話で、トークン数の例として国会図書館や東大図書館の書籍数との比較をされていたのが、どの程度の学習量なのかが理解でき、良かった。', 'プレゼンターの方の声がとても聞き取りやすかったです。', 'まずは全体像をしっかりお伝えいただけた点が大変よかった。', '第5回のSupervised Fine-Tuningの説明は理解できなかった。', 'RLHFやMoEなどは名前は知っていたが、どのような手法なのかイメージできていなかったので、非常に勉強になりました。', '特になし', '次回からの詳細な内容を楽しみにしております。', '各回でどのようなことをするのかがわかりやすかった．知らなかったテクニカルタームもいっぱい知ることができた．\\\\nまた，日本語のLLMの取り巻く状況については，詳しく知ることができた．', '今回のLLM講座の全体像を丁寧に説明いただけたので、LLM初学者の自分でもよく理解できた点が良かったと考えます。', '講義で特によかった部分は、実際の応用例を通じて大規模言語モデル（LLM）の力を実感できた点です。例えば、医療分野での診断支援や、カスタマーサービスでの自動応答システムなど、具体的な事例を交えて説明されました。また、最新の研究成果や技術トレンドについても詳しく解説され、今後の可能性についてのディスカッションが非常に興味深かったです。', '「講座範囲内/外」の意味が若干わかりにくいように感じましたので解説をいただけると助かります。例えば、講座で説明されている内容をより嚙み砕いて知りたい場合は「範囲内」、講座で言及されたトピックについて発展的な内容を知りたい場合は「範囲外」という理解でよろしいでしょうか（それとも後者も「範囲内」でしょうか）。また、範囲内/外により何が変わるのか（参照するDBが変わるのでしょうか）、間違うとどうなるのか（精度が落ちる？）も気になりました。', '自分の目的は、休まず講義を受講して修了証をゲットしたいと考えております。コンペで上位を狙うつもりはありませんが、ファインチューニングの個人演習が不安で心配の気持ち（パイソンでケラスデータセットのCIFAR10や100を使ってCNNの畳み込みニューラルを１から記述するとかですと、いまの自分のレベルですと今からかなり準備が必要と感じています）でいっぱいです、、、自分は、人よりも、のろまなので、今からファインチューニングの勉強をして修了証を取れるように、落第しないようにがんばるには、どうしたらよいのか、分からない状況で困っています。', '・はじめに今後の講義でどんなことを学ぶのかを解説して頂けたので、今後の講義に向けた準備に役立つと思いました。\\\\n・話すスピードが心地よく、頭に入ってきやすかったです。', '全体が俯瞰して理解できた', '以前、貴学の講座によりDLを学んだときはCNN（画像）に重点を置いて学びましたが、あっという間に時代が進み、世の中の興味は生成AI、特にLLMの利活用にシフトしてしまいました。自身の先見性のなさに嫌気を感じつつ、一から学び直すつもりで受講させていただいております。どうぞ、よろしくお願いいたします。', '特にありません。', '特にありませんでした。', 'LLMに必要な学習データと計算量の規模感を数字でわかったこと。', '休憩を入れてほしい。', '研修関するアナウンスがバラバラなって、メールに記載してるものとSlackに記載してる内容も異なります。\\\\n可能なら、ルール関するアナウンス内容を同一して欲しいです\\\\n練習notebookを実行するために、課金しないとダメです。社会人として研修を参加してるので、まとめて料金を請求するような精算しやすい方法の検討をいただけたら助かります', '注意点の一つで，本講義に対するURLはslackで通知しないようにとのことだった．あまりその認識がなかったので講義前にうっかり通知する人がいないか心配になった．よく見ると手引きにも書いてあったが，見逃す人がいるかもしれないので，もう少し目立たせても良いと思った．', '文字数制限の状況やパラメーターの使用方法がわかるとありがたいです。', '出欠アンケートの授業評価ボタンが小さくて押しにくいのでもう少し大きくていいと思います。', 'これからの講義がより楽しみになった。', '声が聞き取りずらい部分があるのでマイクの音質を上げてほしい。', 'Tanukiの開発状況がよくわかり、日本もまだまだこれから勝てる位置にいるのではないかと希望が持てました。\\\\n自身もLLMについてよく学び、役立てることがあれば協力したいと思いました。', '概要を説明していただいたので来週以降のイメージが具体的にわいてきてよかったです。', 'それそれの説明の長さが長すぎることがなく理解しやすかった。', 'LLM講座で今後学習する内容だけではなく、学習する内容を軽く説明してくれて、今後学ぶことをイメージできました。', '実際に開発された「Tanuki」についての講義も併催されており、これからも楽しく学習していきたいです。', '特に、なぜLLMを学ぶのか？という観点で、意義を含めて背景、前提を理解することができた点が良かったと感じました。\\\\nまた、今後の講義についても一度俯瞰して把握しておくことで、イメージが湧くだけでなく、事前の学習にも役立つというメリットもあると感じています。', 'LLMの基本概念から最新の応用例まで、幅広くかつ体系的に説明されていた点がよかったです。\\\\n特に、なぜ今言語モデルが注目されているのかという観点から、モデルの巨大化、Promptingによる汎用性、他領域への影響という3つの要因を挙げて解説されていた部分が印象的でした。\\\\nまた、具体的な事例や図表を交えて説明されていたので、理解が深まりました。', '教材の保存場所（googleドライブのアドレスを聞く意図）を聞いたところ、少し的外れな回答でした。受講者の前提条件（LLM2024の講座を受けており、こういう情報をインプット済）の情報があると、精度が良くなりそうな印象を受けました。', 'LLMの評価指標については未だ理解が浅く、どのような基準や指標で評価しているのか不明なので、後日の講義でしっかり勉強したいと思います。', '特になし', '本来であれば難解なことを判り易く説明していただきました。プレゼン中の引用元URLが役に立ちます。', '全体像がつかめてよかった', '本日もありがとうございました。講義を生かせるように努めます。', 'とても分かりやすかったです。ありがとうございました。', '日本発LLMの重要性、必要性についてもどこかでお聞きできればと思っています（アーカイブ受講のため質問できませんでした）', 'これからは、具体的な内容に入っていくかと思いますが、分かりやすく教えて頂けると非常に助かります。']\n",
      "dangerous_comments:['声が聞き取りやすく、説明も平易であったため、事前知識が少ない状態でも理解しながら講義を受講することができました。', 'まずは全体像をしっかりお伝えいただけた点が大変よかった。']\n",
      "{0: 1277, 2: 40, 1: 38, 3: 51, 5: 28, 4: 14}\n"
     ]
    }
   ],
   "source": [
    "positive_comments = []\n",
    "positive_counts = 0\n",
    "negative_comments = []\n",
    "negative_counts = 0\n",
    "neutral_counts = 0\n",
    "topic_counts = {}\n",
    "important_comments = []\n",
    "dangerous_comments = []\n",
    "\n",
    "for i in range (df.shape[0]//300 + 1):\n",
    "    \n",
    "    comments_batch = comments[i*300:(i+1)*300]\n",
    "    if len(comments_batch) == 0:\n",
    "        continue\n",
    "\n",
    "    results = classify_comments(comments_batch)\n",
    "\n",
    "    if results and 'classifications' in results:\n",
    "        for classification in results['classifications']:\n",
    "            # sentimentの値と、元のコメントのインデックス番号を取得\n",
    "            sentiment = classification['sentiment']\n",
    "            topic = classification['topic']\n",
    "            comment_index = classification['comment_index']\n",
    "            \n",
    "            # 元のコメントを取得\n",
    "            original_comment = comments_batch[comment_index]\n",
    "\n",
    "            # sentimentの値に応じて、各リストにコメントを追加\n",
    "            if sentiment == 0:  \n",
    "                positive_comments.append(original_comment)\n",
    "                positive_counts += 1\n",
    "            elif sentiment == 1:  \n",
    "                negative_comments.append(original_comment)\n",
    "                negative_counts += 1\n",
    "            else:\n",
    "                neutral_counts += 1\n",
    "\n",
    "            topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "\n",
    "    if results and 'important_comments' in results:\n",
    "        for i in results['important_comments']:\n",
    "            important_comments.append(comments_batch[i])\n",
    "\n",
    "    if results and 'dangerous_comments' in results:\n",
    "        for i in results['dangerous_comments']:\n",
    "            dangerous_comments.append(comments_batch[i])\n",
    "\n",
    "\n",
    "    \n",
    "print(\"--- 分析結果の例 ---\")\n",
    "print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "print(f'positive comments:{positive_comments}')\n",
    "print(f'negative comments:{negative_comments}')\n",
    "print(f'important_commnts:{important_comments}')\n",
    "print(f'dangerous_comments:{dangerous_comments}')\n",
    "print(topic_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd02f60",
   "metadata": {},
   "source": [
    "### posi,negaの集約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6bae356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ポジティブなコメントの要約:\n",
      "  - LLMの全体像と歴史的背景、最新の動向を分かりやすく説明してくれた\n",
      "  - 各講義の概要を事前に説明することで、今後の学習計画を立てやすくなった\n",
      "  - 具体的な事例や図表が多く使用され、理解を深めるのに役立った\n",
      "  - 講師の話し方が聞きやすく、テンポも適切だった\n",
      "  - 質疑応答のチャットボットシステムが便利で、多くの質問に回答されていた\n",
      "\n",
      "========================================\n",
      "\n",
      "❌ ネガティブなコメントの要約:\n",
      "  - 講義時間が長く、途中で集中力が途切れたため、休憩時間が必要\n",
      "  - 専門用語が多く、事前知識がないと理解が難しい部分があった\n",
      "  - 資料と講義内容に差異があった\n",
      "  - OmniCampusやSlackの情報が散らばっており、使いにくい\n",
      "  - チャットボットのUI/UXの改善が必要（検索機能、質問の分類、回答の精度など）\n"
     ]
    }
   ],
   "source": [
    "summaries = summarize_comments(positive_comments, negative_comments)\n",
    "\n",
    "# --- 結果をきれいに表示 ---\n",
    "if summaries:\n",
    "    print(\"✅ ポジティブなコメントの要約:\")\n",
    "    for item in summaries.get(\"positive_summary\", []):\n",
    "        print(f\"  - {item}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    print(\"❌ ネガティブなコメントの要約:\")\n",
    "    for item in summaries.get(\"negative_summary\", []):\n",
    "        print(f\"  - {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260a080",
   "metadata": {},
   "source": [
    "### 重要コメントの集約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eac6912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 優先度順の要約 ---\n",
      "優先度 1: Zoomリンクの共有方法やSlackの情報整理、アンケートの改善など、受講者体験を阻害する複数のUI/UX上の問題が報告されている。 (関連コメントNo: [0, 48, 50, 51, 52, 55, 75, 78, 80, 85])\n",
      "優先度 2: 事前資料の提供時期や量、専門用語の多さ、講義スピードなど、学習内容の理解を妨げる複数の要因が指摘されている。 (関連コメントNo: [3, 14, 17, 26, 71, 79, 81])\n",
      "優先度 3: 字幕表示機能の追加など、聴覚障がい者を含む全ての受講者にとってアクセシビリティを向上させる必要がある。 (関連コメントNo: [53])\n",
      "優先度 4: 資料投影画面の録画提供、演習問題の解答例提供など、学習効果を高めるための具体的な改善要望が複数寄せられている。 (関連コメントNo: [4, 21])\n",
      "優先度 4: 質疑応答の機能改善（いいね機能追加、キーワード検索機能追加など）に関する要望が複数寄せられている。 (関連コメントNo: [49, 50])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def summarize_important_comments(comments: list[dict]) -> dict | None:\n",
    "    \"\"\"\n",
    "    重要コメントのリストを受け取り、優先順位付けされた要約を返す関数\n",
    "    \"\"\"\n",
    "    if not comments:\n",
    "        print(\"要約する重要コメントがありません。\")\n",
    "        return None\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    あなたは、大量の顧客フィードバックを分析し、改善のためのアクションアイテムを優先順位付けする、経験豊富なプロダクトマネージャーです。\n",
    "\n",
    "これから、重要だと判断されたコメントのリストをJSON形式で受け取ります。あなたのタスクは、これらのコメントを総合的にレビューし、最も対応すべき優先度の高い課題をトップ5まで特定し、要約してランキング形式で出力することです。\n",
    "\n",
    "# 分析と優先順位付けのルール\n",
    "1.  **内容の理解とグルーピング**: まず、全てのコメントを読み、内容の似ているもの（例：「音声が途切れる」「音が聞こえない」）を心の中でグループ化します。\n",
    "2.  **優先度の決定**: 以下の基準に従って、各グループの優先度を決定してください。\n",
    "    * **優先度1 (最高)**: **緊急性の高い問題**。サービスの利用を妨げる致命的な障害や、重大な間違いの指摘（例：「動画が再生できない」「ログインできない」）。\n",
    "    * **優先度2 (高)**: **頻出の意見・要望**。複数のユーザーから繰り返し指摘されている問題や改善案（例：「もっと演習時間が欲しい」という声が多数）。\n",
    "    * **優先度3 (中)**: **具体的な改善提案**。たとえ一人からの意見でも、具体的で実行可能な改善案（例：「各章の最後にまとめのスライドを追加してほしい」）。\n",
    "    * **優先度4 (低)**: 上記以外の一般的な意見。\n",
    "\n",
    "# 出力フォーマット\n",
    "- 分析結果を以下のJSONフォーマットで出力してください。\n",
    "- `priority`は1から始まる整数、`summary`はその課題や提案を簡潔にまとめた一文です。\n",
    "- `indices`には、その要約の根拠となった元のコメントのインデックス番号をリストで含めてください。\n",
    "- 全体で最大5つの項目を、優先度の高い順に並べてください。\n",
    "- あなたの応答は、解説や前置きなしに、JSONオブジェクトのみにしてください。\n",
    "\n",
    "{\n",
    "  \"ranked_summary\": [\n",
    "    {\n",
    "      \"priority\": 1,\n",
    "      \"summary\": \"動画の再生に関する技術的な問題が発生しており、緊急の対応が必要です。\",\n",
    "      \"indices\": [2, 18, 45]\n",
    "    },\n",
    "    {\n",
    "      \"priority\": 2,\n",
    "      \"summary\": \"講義の進行速度が速すぎるため、ついていけないという意見が多数寄せられています。\",\n",
    "      \"indices\": [6, 11, 23, 31]\n",
    "    },\n",
    "    {\n",
    "      \"priority\": 3,\n",
    "      \"summary\": \"演習問題の解答例を提供してほしいという具体的な要望があります。\",\n",
    "      \"indices\": [22]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "---\n",
    "それでは、以下の重要コメントリストを分析し、優先順位をつけて要約してください。\n",
    "【本番】\n",
    "\"\"\"\n",
    "    comment_list_str = json.dumps(comments, ensure_ascii=False)\n",
    "\n",
    "    # プレースホルダーを置換\n",
    "    # f-stringを使うと{}のエスケープが面倒なので、ここでは .format() を使います\n",
    "    # プロンプト内の例示JSONの { と } は {{ }} にエスケープしておく必要があります\n",
    "    prompt = prompt_template + comment_list_str\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        clean_response = response.text.strip().replace('```json', '').replace('```', '').strip()\n",
    "        summary = json.loads(clean_response)\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        return None\n",
    "\n",
    "ranked_summary = summarize_important_comments(important_comments)\n",
    "\n",
    "\n",
    "if ranked_summary:\n",
    "    print(\"--- 優先度順の要約 ---\")\n",
    "    for item in ranked_summary.get(\"ranked_summary\", []):\n",
    "        print(f\"優先度 {item['priority']}: {item['summary']} (関連コメントNo: {item['indices']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
